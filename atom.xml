<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Irving's blog]]></title>
  <link href="http://irvingpop.github.io/atom.xml" rel="self"/>
  <link href="http://irvingpop.github.io/"/>
  <updated>2015-05-18T13:36:15-07:00</updated>
  <id>http://irvingpop.github.io/</id>
  <author>
    <name><![CDATA[Irving Popovetsky]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tuning the Chef Server for Scale]]></title>
    <link href="http://irvingpop.github.io/blog/2015/04/20/tuning-the-chef-server-for-scale/"/>
    <updated>2015-04-20T13:51:36-07:00</updated>
    <id>http://irvingpop.github.io/blog/2015/04/20/tuning-the-chef-server-for-scale</id>
    <content type="html"><![CDATA[<p>In Chef&rsquo;s Customer Engineering team we are frequently asked for advice on tuning the Chef Server for high-scale situations. Below is a summary of what we generally tell customers. Note that these tuning settings are specific to Chef Server 12, which is the recommended version for any customer who cares about the performance of their Chef server.</p>

<!-- more -->


<h2>General Advice</h2>

<h3>Understand the OSS components that make up the Chef Server</h3>

<p>A good way to think about the Chef server is as a collection of microservices components underpinned by OSS software:</p>

<ul>
<li>Nginx (openresty)</li>
<li>PostgreSQL</li>
<li>Solr</li>
<li>RabbitMQ</li>
<li>Redis</li>
<li>Chef</li>
<li>Erlang/OTP</li>
<li>Ruby</li>
<li><a href="http://smarden.org/runit/">Runit</a></li>
<li>The Linux Kernel

<ul>
<li>LVM</li>
<li>Storage subsystem</li>
<li>Network stack</li>
</ul>
</li>
</ul>


<p>It&rsquo;s important to understand the performance characteristics, monitoring and troubleshooting of these components.  Especially Postgres, Solr, RabbitMQ, Runit and Linux systems in general. It&rsquo;s worth noting that the Chef server core is Open Source, and all if its code can be examined <a href="https://github.com/chef/chef-server">on Github</a></p>

<p>Because these components are glued together using Chef, it&rsquo;s highly recommended that you familiarize yourself with the <a href="https://github.com/chef/opscode-omnibus/tree/master/files/private-chef-cookbooks">cookbooks that configure the Chef server when you run <code>chef-server-ctl reconfigure</code></a></p>

<h3>Have good monitoring in place</h3>

<p>We don&rsquo;t provide prescriptive monitoring guidance at this time, but here&rsquo;s our advice:</p>

<ul>
<li>Use existing Open source software (Sensu, Nagios, etc) to collect metrics and test the health of the OSS components.  This should be fairly straightforward to set up.

<ul>
<li>Use <a href="http://dalibo.github.io/pgbadger/">pgBadger</a> for Postgres log analysis</li>
<li>Install the <a href="https://www.rabbitmq.com/management.html">RabbitMQ Management Plugin</a> for detailed monitoring of RabbitMQ</li>
</ul>
</li>
<li>Configure your monitoring systems and load balancers to query the Health status endpoint of erchef (<a href="https://mychefserver/_status">https://mychefserver/_status</a>)</li>
<li>Run a graphite server. erchef will send detailed statistics if you set the following in your <code>chef-server.rb</code> file:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">folsom_graphite</span><span class="o">[</span><span class="s1">&#39;enable&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="kp">true</span>
</span><span class='line'><span class="n">folsom_graphite</span><span class="o">[</span><span class="s1">&#39;host&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s1">&#39;graphite.mycompany.com&#39;</span>
</span><span class='line'><span class="n">folsom_graphite</span><span class="o">[</span><span class="s1">&#39;port&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">2003</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Use Splunk or Logstash to collect and analyze your Chef server logs.

<ul>
<li>You can collect and graph useful performance data by graphing <code>/var/log/opscode/opscode-erchef/requests.log.N</code>, <code>/var/log/opscode/oc_bifrost/requests.log.N</code> and <code>/var/log/opscode/opscode-reporting/requests.log.N</code></li>
<li>Each request line will show (in ms) various performance counter. For example: <code>req_time=20; rdbms_time=2; rdbms_count=3; authz_time=5; authz_count=1;</code></li>
</ul>
</li>
</ul>


<h3>Think about API requests per second rather than node counts</h3>

<p>A very common measurement for the size of Chef servers/clusters is the number of nodes they serve. However, this number is not terribly useful because of other elements that can cause very wide variation. Namely:</p>

<ul>
<li>The interval and splay of Chef client runs

<ul>
<li>1000 nodes every hour == 500 nodes every 30 minutes</li>
<li>Insufficient splay can cause a &ldquo;stampede condition&rdquo; on the Chef server. Splay should be equal to the interval in order to get maximum smoothness of request load.</li>
</ul>
</li>
<li>The number and complexity of search requests and databag fetches performed during each Chef run</li>
<li>The number of cookbooks depended on for each Chef run. More cookbooks adds loading to the depsolver and also to the Bookshelf service which serves cookbooks</li>
<li>The size of node data, which we&rsquo;ve seen range from 32kb to 5MB (the default maximum is 1MB but can be increased). This adds load to the indexing service (opscode-expander) as well as to Solr</li>
</ul>


<p>Although it&rsquo;s not perfect, we&rsquo;ve found that a good rule of thumb for examining active Chef servers is the number of API requests per second aggregated across the entire cluster. We&rsquo;ve found that clusters which sustained higher than 125 API RPS started to experience occasional errors.</p>

<h3>DRBD: Don&rsquo;t do it</h3>

<p>In the field we&rsquo;ve found that DRBD has a negative impact on performance and availability of Chef server clusters. Specifically:</p>

<ul>
<li>Because DRBD uses synchronous replication, a block is not considered &ldquo;committed to disk&rdquo; until it has been confirmed by both nodes in the cluster. This adds significant latency to each IOP.</li>
<li>DRBD&rsquo;s bandwidth is limited by the network throughput between the nodes. Dedicated cross-over links are not possible in all scenarios (for example VMs) which leads to low and inconsistent throughput.</li>
<li>DRBD resyncs can take a very long time and greatly impact performance while running.</li>
<li>Although DRBD protects against hardware failure, it does a very poor job of protecting against many classes of software failure. For example, a corrupt database is replicated whole to the other node, so failing over will not correct the system.</li>
</ul>


<h3>Beware LVM snapshots impact on performance</h3>

<p>LVM is generally recommended for storing all Chef Server data (<code>/var/opt/opscode</code> in standalone/tier installs and <code>/var/opt/opscode/drbd/data</code> in HA installs) because it provides the ability to expand disks on the fly and create crash-consistent snapshots.</p>

<p>However it&rsquo;s important to know that as LVM snapshots increase in size it is very detrimental to performance:</p>

<ul>
<li><a href="http://www.percona.com/blog/2013/07/09/lvm-read-performance-during-snapshots/">http://www.percona.com/blog/2013/07/09/lvm-read-performance-during-snapshots/</a></li>
<li><a href="http://www.percona.com/blog/2009/02/05/disaster-lvm-performance-in-snapshot-mode/">http://www.percona.com/blog/2009/02/05/disaster-lvm-performance-in-snapshot-mode/</a></li>
</ul>


<p>Therefore it is recommend that snapshots are used to create consistent backups, but are immediately deleted after they are no longer needed.</p>

<h2>Chef Server tuning tips</h2>

<h3>Server sizing</h3>

<p><strong>Chef Server frontends:</strong></p>

<ul>
<li>Frontends run stateless services only (erchef, bifrost, reporting, manage) and can be scaled horizontally.</li>
<li>They are almost always CPU bound, and only suffer memory or disk pressure during fault scenarios (typically because of backend issues).</li>
<li>A good starting point for frontends is 4 CPU cores and 8 GB RAM. Disk on frontends does not matter.</li>
</ul>


<p><strong>Chef server backends:</strong></p>

<ul>
<li>Backends mix a number of disk, memory and CPU bound services (Postgres, Solr, RabbitMQ, Expander)</li>
<li>A good starting point for backends is 8 CPU cores and 32 GB of RAM.</li>
<li>Flash-based storage is highly recommend, combined with the XFS filesystem and LVM.</li>
</ul>


<h3>chef-server.rb tuning settings</h3>

<p><strong>Database pooling:</strong></p>

<p>In the erlang OTP process model, the number of workers is limited by the size of the database connection pool (default 20). Increasing the database pool allows for more workers, but puts added memory pressure on the database service.</p>

<p>In order to handle the greater number of connections, you must also increase the Postgres <code>max_connections</code> value. This value must consider an erchef, bifrost and reporting process connecting from each frontend, plus an extra 20% for breathing room.</p>

<p>Suggested values for a high-performing cluster with 4-6 frontends:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">postgresql</span><span class="o">[</span><span class="s1">&#39;max_connections&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">1024</span>
</span><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;db_pool_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">60</span>
</span><span class='line'><span class="n">oc_bifrost</span><span class="o">[</span><span class="s1">&#39;db_pool_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">60</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>Erchef to bifrost http connection pool:</strong>
erchef also maintains a pool of http connections to bifrost, the authz service.  It&rsquo;s important to raise the initial and maximum number of connections with respect to the database pool sizes.</p>

<p><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">oc_chef_authz</span><span class="o">[</span><span class="s1">&#39;http_init_count&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">100</span>
</span><span class='line'><span class="n">oc_chef_authz</span><span class="o">[</span><span class="s1">&#39;http_max_count&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">200</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>Erchef depsolver and keygen tuning:</strong>
Two expensive computations that erchef must perform are the depsolver (a Ruby process which solves the cookbook dependencies) as well as the client key generator (which can be hit hard when large fleets of chef nodes are provisioned). Note that Chef 12 clients default to client-side key generation and you probably only need to adjust the keygen value if you still use Chef 11 clients.</p>

<p>Suggested values:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;depsolver_worker_count&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># should equal the number of CPU cores</span>
</span><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;depsolver_timeout&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">120000</span>
</span><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;keygen_cache_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">1000</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>Nginx cookbook caching:</strong>
A new feature in Chef Server 12.0.4 is <a href="https://www.chef.io/blog/2015/02/18/cookbook-caching/">Nginx cookbook caching</a>. This takes load off of the backend Bookshelf service by storing cookbook files in Nginx.</p>

<p>Suggested values:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;nginx_bookshelf_caching&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s2">&quot;:on&quot;</span>
</span><span class='line'><span class="n">opscode_erchef</span><span class="o">[</span><span class="s1">&#39;s3_url_expiry_window_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s2">&quot;100%&quot;</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>PostgreSQL tuning:</strong>
We already tune PostgreSQL memory settings to sane values based on the backend&rsquo;s phyiscal RAM. For example, <code>effective_cache_size</code> is set to 50% of RAM, and <code>shared_buffers</code> to 25% of physical RAM.</p>

<p>To handle the heavy write load on large clusters, it is recommended to tune the checkpointer per [<a href="https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server">https://wiki.postgresql.org/wiki/Tuning_Your_PostgreSQL_Server</a>]</p>

<p>Suggested values:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">postgresql</span><span class="o">[</span><span class="s1">&#39;checkpoint_segments&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">64</span>
</span><span class='line'><span class="n">postgresql</span><span class="o">[</span><span class="s1">&#39;checkpoint_completion_target&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">0</span><span class="o">.</span><span class="mi">9</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><strong>Solr JVM tuning:</strong>
By default we compute Solr&rsquo;s JVM heap size to be either 25% of system memory or 1024MB, whichever is smaller. Large chef server clusters should increase this value to smaller of 25% of system memory or 4096MB.</p>

<p>Suggested values:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">opscode_solr4</span><span class="o">[</span><span class="s1">&#39;heap_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">4096</span>
</span><span class='line'><span class="n">opscode_solr4</span><span class="o">[</span><span class="s1">&#39;new_size&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="mi">256</span>
</span></code></pre></td></tr></table></div></figure></p>

<p><em>WARNING: It is not recommended to use a JVM heap_size above 8GB, at that level the cost of Garbage Collection becomes too high and impacts performance worse than using a smaller heap</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Your Private Supermarket Server]]></title>
    <link href="http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server/"/>
    <updated>2015-04-07T15:48:34-07:00</updated>
    <id>http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server</id>
    <content type="html"><![CDATA[<p><em>This is an updated version of the previous post from August, 2014: <a href="https://www.chef.io/blog/2014/08/29/getting-started-with-oc-id-and-supermarket/">Getting started with oc-id and Supermarket</a></em></p>

<p>Chef Server 12 includes <a href="https://github.com/chef/oc-id">oc-id</a>, the OAuth2 service that powers <a href="https://id.chef.io/">id.chef.io</a>.  After upgrading to this release, Chef customers can now run their own Supermarket service behind a firewall.</p>

<!-- more -->


<h2>oc-id setup on your Chef Server:</h2>

<p><em>You must be logged in to your Chef server via ssh and elevated to an admin user level for the following steps</em></p>

<ol>
<li><p>Add the following setting to your <code>/etc/opscode/chef-server.rb</code> configuration file:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">oc_id</span><span class="o">[</span><span class="s1">&#39;applications&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>  <span class="s1">&#39;supermarket&#39;</span> <span class="o">=&gt;</span> <span class="p">{</span>
</span><span class='line'>    <span class="s1">&#39;redirect_uri&#39;</span> <span class="o">=&gt;</span> <span class="s1">&#39;https://supermarket.mycompany.com/auth/chef_oauth2/callback&#39;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>run <code>chef-server-ctl reconfigure</code></p></li>
<li><p>After the reconfigure, you will find the OAuth2 data in <code>/etc/opscode/oc-id-applications/supermarket.json</code>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;supermarket&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;uid&quot;</span><span class="p">:</span> <span class="s2">&quot;0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;secret&quot;</span><span class="p">:</span> <span class="s2">&quot;17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;redirect_uri&quot;</span><span class="p">:</span> <span class="s2">&quot;https://supermarket.mycompany.com/auth/chef_oauth2/callback&quot;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Note the <code>uid</code> and <code>secret</code> values from this file, you will need them for the next stage.</p>

<p><em>You can add as many oc-id applications as you wish to the chef-server.rb configuration, it will create one file per application</em></p>

<h2>Running your Private Supermarket server in Test Kitchen</h2>

<p><em>Note: We will not use the community Supermarket cookbook, because at this time it installs Supermarket from source.  Instead, we will us an Omnibus package to install</em></p>

<p>In the spirit of &ldquo;code as documentation&rdquo; I&rsquo;ve provided a simple cookbook and test-kitchen configuration for testing Supermarket Omnibus packages. These packages are downloaded from <a href="https://packagecloud.io/chef/stable">https://packagecloud.io/chef/stable</a></p>

<ol>
<li><p>Download a copy of the [supermarket-omnibus-cookbook]<a href="https://github.com/irvingpop/supermarket-omnibus-cookbook">https://github.com/irvingpop/supermarket-omnibus-cookbook</a>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/irvingpop/supermarket-omnibus-cookbook.git supermarket-omnibus-cookbook
</span><span class='line'><span class="nb">cd </span>supermarket-omnibus-cookbook
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a <code>.kitchen.local.yml</code> file, to set your oc-id attributes (as captured in step 3 above)
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">suites</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">default</span>
</span><span class='line'>    <span class="l-Scalar-Plain">run_list</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">recipe[supermarket-omnibus-cookbook::default]</span>
</span><span class='line'>    <span class="l-Scalar-Plain">attributes</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">supermarket_omnibus</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_server_url</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">https://chefserver.mycompany.com</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_app_id</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_secret</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_verify_ssl</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">false</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install the <code>vagrant-hostsupdater</code> plugin, this will automatically add the names of your machines to your /etc/hosts file. This is important for oauth2, which cares about host names. The <code>redirect_uri</code> value you entered in to your oc-id configuration reflects this name.
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant plugin install vagrant-hostsupdater
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Start your Supermarket instance and test it
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>kitchen converge default-centos-66 <span class="o">&amp;&amp;</span> kitchen verify default-centos-66
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Go to your your Supermarket server and log in as a Chef user: <a href="https://default-centos-66">https://default-centos-66</a></p></li>
<li><p>Upon login, you should see:
<img src="https://www.getchef.com/blog/wp-content/uploads/2014/08/oc-id5-1024x343.png"></p></li>
</ol>


<h2>Uploading your first cookbook to Supermarket</h2>

<ol>
<li>Install the <a href="https://github.com/chef/knife-supermarket">knife-supermarket</a> gem. In ChefDK:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>chef gem install knife-supermarket
</span></code></pre></td></tr></table></div></figure></li>
<li>In your <code>knife.rb</code> file, add a setting for the supermarket server:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">knife</span><span class="o">[</span><span class="ss">:supermarket_site</span><span class="o">]</span> <span class="o">=</span> <span class="s1">&#39;https://default-centos-66&#39;</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>To resolve any SSL errors, fetch and verify the Supermarket server&rsquo;s SSL certificate:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>knife ssl fetch https://default-centos-66
</span><span class='line'>knife ssl check https://default-centos-66
</span></code></pre></td></tr></table></div></figure></li>
<li>Upload your cookbook to Supermarket
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>knife supermarket share mycookbook <span class="s2">&quot;Other&quot;</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h2>Running Supermarket in Production</h2>

<p>Supermarket is still in early stages and does not have official Support from Chef, HA, backup tools, etc.  Although several of our key customers are running Supermarket in prod, they are doing it at their own risk.</p>

<p>In general we recommend that you start using small VMs, it&rsquo;s easy to increase your VM size as you need it. Put your <code>/var/opt/supermarket</code> directory on a separate disk and use LVM so that it can be expanded.</p>

<h3>Your Wrapper Cookbook attributes</h3>

<p>We recommend that you use use a wrapper cookbook with role recipes to deploy Supermarket.</p>

<p>All of the keys under <code>node['supermarket_omnibus']</code> are written out as <code>/etc/supermarket/supermarket.json</code>.  You can add others as you see fit to override the defaults specified in the <a href="https://github.com/chef/omnibus-supermarket/blob/master/cookbooks/omnibus-supermarket/attributes/default.rb">supermarket Omnibus package</a>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">default</span><span class="o">[</span><span class="s1">&#39;supermarket_omnibus&#39;</span><span class="o">][</span><span class="s1">&#39;chef_server_url&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s1">&#39;https://chefserver.mycompany.com&#39;</span>
</span><span class='line'><span class="n">default</span><span class="o">[</span><span class="s1">&#39;supermarket_omnibus&#39;</span><span class="o">][</span><span class="s1">&#39;chef_oauth2_app_id&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s1">&#39;14dfcf186221781cff51eedd5ac1616&#39;</span>
</span><span class='line'><span class="n">default</span><span class="o">[</span><span class="s1">&#39;supermarket_omnibus&#39;</span><span class="o">][</span><span class="s1">&#39;chef_oauth2_secret&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="s1">&#39;a49402219627cfa6318d58b13e90aca&#39;</span>
</span><span class='line'><span class="n">default</span><span class="o">[</span><span class="s1">&#39;supermarket_omnibus&#39;</span><span class="o">][</span><span class="s1">&#39;chef_oauth2_verify_ssl&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="kp">false</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3>Scaling the system</h3>

<p>Supermarket is a Ruby on Rails app with a Postgres backend, and typical RoR scaling rules apply.  If you wish to run Supermarket in a scale-out or HA mode, you can do this by building our your own back-end components:</p>

<ul>
<li><strong>Database:</strong> Build a separate PostgreSQL 9.3+ server (or HA pair). Please note that the following Postgres extensions must be installed and loaded: <code>pgpsql</code> and <code>pg_trgm</code></li>
<li><strong>Cookbook Storage</strong> Cookbook tarballs are stored by default in <code>/var/opt/supermarket/data</code>. You can change this to use Amazon S3 (recommended) or an <a href="http://stackoverflow.com/questions/10574909/is-there-an-open-source-equivalent-to-amazon-s3">S3-compatible service</a>. If those are not an option you can symlink this directory to shared storage (e.g. NFS) although this has not been fully tested against race conditions.</li>
<li><strong>(Optional) Caching Service:</strong> Supermarket uses Redis as its caching service. You can safely run one Redis instance per Supermarket app server, or you can choose to run a Redis 2.8+ server or HA pair.</li>
</ul>


<h2>Troubleshooting &amp; FAQ</h2>

<h3>Incorrect redirect URL</h3>

<p>The redirect URL specified in oc-id <strong>MUST</strong> match the hostname of the Supermarket server. Also, you must get the URI correct (/auth/chef_oauth2/callback). If these are not true, you will recieve an error message like:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="no">The</span> <span class="n">redirect</span> <span class="n">uri</span> <span class="n">included</span> <span class="n">is</span> <span class="ow">not</span> <span class="n">valid</span><span class="o">.</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3>Supermarket server cannot reach oc-id, throws 500 error during login</h3>

<p>The Supermarket server must be able to reach (via https) the specified <code>chef_server_url</code> - it does this during OAuth2 negotation. The most common problems are name resolution and firewall rules.</p>

<h3>Where can I find the code to Supermarket?</h3>

<ul>
<li>Supermarket the rails application is <a href="https://github.com/chef/supermarket">located here</a>

<ul>
<li>All Supermarket <a href="https://github.com/chef/supermarket/issues">issues should be reported there</a></li>
</ul>
</li>
<li>The code which builds Supermarket into an Omnibus package is <a href="https://github.com/chef/omnibus-supermarket">located here</a>

<ul>
<li>The cookbook that is run when during <code>supermarket-ctl reconfigure</code> is <a href="https://github.com/chef/omnibus-supermarket/tree/master/cookbooks/omnibus-supermarket">located within this repo</a></li>
<li>You can build your own Omnibus packages by following <a href="https://github.com/chef/omnibus-supermarket#kitchen-based-build-environment">the instructions in the README.md</a></li>
</ul>
</li>
</ul>


<h3>How do I enable rails application debug logging?</h3>

<p>There is a known issue with the Supermarket omnibus package that rails messages are not logged. To fix that requires a manual change at the moment. On your supermarket server, edit this file: <code>/opt/supermarket/embedded/service/supermarket/config/environments/production.rb</code>, change line 46 (<code>config.log_level = :warn</code>) to look like:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="no">Logger</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="s1">&#39;/var/log/supermarket/rails/rails.log&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">level</span> <span class="o">=</span> <span class="s1">&#39;DEBUG&#39;</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">log_level</span> <span class="o">=</span> <span class="ss">:debug</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Then restart the rails service by running
<code>supermarket-ctl restart rails</code></p>

<h3>How does this OAuth2 stuff work anyway?</h3>

<p>Here&rsquo;s a simplified description of OAuth2:  <a href="https://aaronparecki.com/articles/2012/07/29/1/oauth2-simplified">https://aaronparecki.com/articles/2012/07/29/1/oauth2-simplified</a></p>

<ol>
<li>When you visit supermarket at <a href="https://supermarket">https://supermarket</a><a href="https://supermarket/">https://supermarket/</a> and click login, that login redirects you to <a href="https://chef-server/oc-id">https://chef-server/oc-id</a></li>
<li><a href="https://chef-server/oc-id">https://chef-server/oc-id</a> then redirects you back to <a href="https://supermarket/auth/endpoint">https://supermarket/auth/endpoint</a> once you are confirmed as authed</li>
<li>Supermarket talks to chef-server/oc-id to verify the token it just received by making an https call to the chef server</li>
</ol>


<h3>Contacting packagecloud fails if I&rsquo;m behind a proxy</h3>

<p>No problem!  Add the following to your <code>.kitchenl.local.yml</code> file:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">provisioner</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">chef_zero</span>
</span><span class='line'>  <span class="l-Scalar-Plain">solo_rb</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">http_proxy</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">http://192.168.1.1</span>
</span><span class='line'>    <span class="l-Scalar-Plain">https_proxy</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">http://192.168.2.2</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3>Test kitchen is slow because it has to download/install the Chef Omnibus client package every time</h3>

<p>Here&rsquo;s a few tips to speed it up:</p>

<ol>
<li>Tell test-kitchen to cache the Omnibus installer (put this in your <code>.kitchen.local.yml</code> file):
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">provisioner</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">chef_zero</span>
</span><span class='line'>  <span class="l-Scalar-Plain">chef_omnibus_install_options</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">-d /tmp/vagrant-cache/vagrant_omnibus</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Cache yum repos like packagecloud using the vagrant-cachier plugin.  First run <code>vagrant plugin install vagrant-cachier</code>, then create a <code>$VAGRANT_HOME/Vagrantfile</code> that looks like so:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="no">Vagrant</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="s1">&#39;some-box&#39;</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">Vagrant</span><span class="o">.</span><span class="n">has_plugin?</span><span class="p">(</span><span class="s2">&quot;vagrant-cachier&quot;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="ss">:box</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:chef</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:apt</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:yum</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:gem</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Experimenting With Redhat Cluster Suite 7]]></title>
    <link href="http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments/"/>
    <updated>2015-03-01T14:48:34-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments</id>
    <content type="html"><![CDATA[<h2>Description</h2>

<p>Source code at: <a href="https://github.com/irvingpop/rhcs7">https://github.com/irvingpop/rhcs7</a></p>

<p>I created a Vagrant-based configuration that brings up two CentOS 7 nodes that share a cluster disk with CLVM.
Based on:  <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html</a>
with clarification from: <a href="http://www.davidvossel.com/wiki/index.php?title=HA_LVM">http://www.davidvossel.com/wiki/index.php?title=HA_LVM</a></p>

<p>Ultimate goal: Find a way to couple Redhat clustering with Chef Server to create a more robust HA stack.</p>

<!-- more -->


<h2>Notes</h2>

<ul>
<li>Redhat clustering is HARD and not for the faint of heart.  It&rsquo;s not nearly as well documented as it should be, there&rsquo;s quite a few closely interconnected services and you should really take time out to understand it on its own before combining it with anything else.

<ul>
<li>Redhat clustering in EL5 and EL6 was pretty awful to operate and hard to understand.  It has been majorly redone in RHEL7 and is now considerably simpler to get going.</li>
</ul>
</li>
<li>This document and repo focus on the <em>shared storage</em> use case, where two nodes have access to the same block device but only one node can actively mount and write to it at a time. The simultaneous access use cases (GFS, OCFS, etc) are not covered here.</li>
<li>Initially I tried to use Virtualbox &ldquo;shareable&rdquo; block devices but this didn&rsquo;t work because it doesn&rsquo;t support the SCSI SPC-3 feature set.  This is important because the only applicable fence agent I could use is fence_scsi, which uses SCSI SPC-3 persistent reservations ( <a href="https://kb.netapp.com/support/index?page=content&amp;id=3012956">https://kb.netapp.com/support/index?page=content&amp;id=3012956</a> )</li>
<li>I switched to using the Linux iSCSI service on a separate node to get SPC-3.  In production you shouldn&rsquo;t use iSCSI.  It will burn your beans, melt your ice cream, and in general disappoint you continuously.  Yes, Fibre Channel is expensive and iSCSI is cheap, but over a decade of storage management experience has taught me that there&rsquo;s a good reason for that.</li>
</ul>


<p>There&rsquo;s two ways to do HA-LVM:
* Exclusive Activation via LVM volume_list filtering: This basically tells LVM not to auto-activate the shared storage, and then uses a tagging system (plus trust in pacemaker) to control exclusive access
* Exclusive Activation via clvmd: This uses the clvmd service (plus a distributed lock manager) to control access to LVM volume groups which are specially tagged as &ldquo;clustered&rdquo;
* Irving&rsquo;s thoughts: clvmd would appear to be the safer way to go because it protects against &ldquo;rogue&rdquo; nodes, but in practice clvmd is really freakin hard to figure out and troubleshoot. Also, the SCSI SPC-3 persistent reservations provide an additional layer of safety.  So I recommend Option 1, the volume_list filtering approach as it gave me far less heartburn in testing.</p>

<h2>Initial setup</h2>

<ol>
<li>Initial configuration of the cluster machines, backend0 and backend1:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant up
</span></code></pre></td></tr></table></div></figure></li>
<li><p>iSCSI server setup
from: <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup</a>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install</span>
</span><span class='line'>yum install -y targetcli
</span><span class='line'><span class="c"># start and enable the service</span>
</span><span class='line'>systemctl start target <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable </span>target
</span><span class='line'>
</span><span class='line'><span class="c"># map the backstore to a physical unformatted block device</span>
</span><span class='line'>targetcli /backstores/block create <span class="nv">name</span><span class="o">=</span>block_backend <span class="nv">dev</span><span class="o">=</span>/dev/sdb
</span><span class='line'>
</span><span class='line'><span class="c"># create an iSCSI target</span>
</span><span class='line'>targetcli /iscsi create iqn.2006-04.com.iscsi-is-awesome:1
</span><span class='line'><span class="c">#  Created target iqn.2006-04.com.iscsi-is-awesome:1</span>
</span><span class='line'><span class="c">#  Created TPG 1.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># create the iSCSI portal (IP listener)</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/portals/ create
</span><span class='line'><span class="c"># Using default IP port 3260</span>
</span><span class='line'><span class="c"># Binding to INADDR_ANY (0.0.0.0)</span>
</span><span class='line'><span class="c"># Created network portal 0.0.0.0:3260.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Map the iSCSI target to the backstore</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/luns/ create /backstores/block/block_backend
</span><span class='line'><span class="c"># Created LUN 0.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># crazy wild-west no-ACL mode, because this is a PoC :)</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/ <span class="nb">set </span>attribute <span class="nv">authentication</span><span class="o">=</span><span class="m">0</span> <span class="nv">demo_mode_write_protect</span><span class="o">=</span><span class="m">0</span> <span class="nv">generate_node_acls</span><span class="o">=</span><span class="m">1</span> <span class="nv">cache_dynamic_acls</span><span class="o">=</span>1
</span><span class='line'><span class="c"># Parameter demo_mode_write_protect is now &#39;0&#39;.</span>
</span><span class='line'><span class="c"># Parameter authentication is now &#39;0&#39;.</span>
</span><span class='line'><span class="c"># Parameter generate_node_acls is now &#39;1&#39;.</span>
</span><span class='line'><span class="c"># Parameter cache_dynamic_acls is now &#39;1&#39;.</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c"># sit back and marvel at your iSCSIs</span>
</span><span class='line'>targetcli ls
</span><span class='line'><span class="c"># o- / ........................................ [...]</span>
</span><span class='line'><span class="c">#  o- backstores ............................. [...]</span>
</span><span class='line'><span class="c">#  | o- block ................. [Storage Objects: 1]</span>
</span><span class='line'><span class="c">#  | | o- block_backend  [/dev/sdb (1.0GiB) write-thru activated]</span>
</span><span class='line'><span class="c">#  | o- fileio ................ [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  | o- pscsi ................. [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  | o- ramdisk ............... [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  o- iscsi ........................... [Targets: 1]</span>
</span><span class='line'><span class="c">#  | o- iqn.2006-04.com.iscsi-is-awesome:1  [TPGs: 1]</span>
</span><span class='line'><span class="c">#  |   o- tpg1 .............. [no-gen-acls, no-auth]</span>
</span><span class='line'><span class="c">#  |     o- acls ......................... [ACLs: 0]</span>
</span><span class='line'><span class="c">#  |     o- luns ......................... [LUNs: 1]</span>
</span><span class='line'><span class="c">#  |     | o- lun0  [block/block_backend (/dev/sdb)]</span>
</span><span class='line'><span class="c">#  |     o- portals ................... [Portals: 1]</span>
</span><span class='line'><span class="c">#  |       o- 0.0.0.0:3260 .................... [OK]</span>
</span><span class='line'><span class="c">#  o- loopback ........................ [Targets: 0]</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>iSCSI client setup
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install and start the iscsi client service</span>
</span><span class='line'>yum install -y iscsi-initiator-utils
</span><span class='line'>systemctl start iscsid.service <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable </span>iscsid.service
</span><span class='line'>
</span><span class='line'><span class="c"># connect to the iscsi server</span>
</span><span class='line'>iscsiadm -m node -o new -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260
</span><span class='line'>
</span><span class='line'><span class="c"># login</span>
</span><span class='line'>iscsiadm -m node -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260 --login
</span><span class='line'><span class="c"># Logging in to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] (multiple)</span>
</span><span class='line'><span class="c"># Login to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] successful.</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Base Cluster Setup</h2>

<ol>
<li>Run the following commands on both nodes
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install clustering packages</span>
</span><span class='line'>yum -y install pcs fence-agents-all lvm2-cluster
</span><span class='line'>
</span><span class='line'><span class="c"># set hacluster user password</span>
</span><span class='line'><span class="nb">echo</span> <span class="s2">&quot;hacluster&quot;</span> <span class="p">|</span> passwd hacluster --stdin
</span><span class='line'>
</span><span class='line'><span class="c"># start clustering services</span>
</span><span class='line'>systemctl start pcsd.service
</span><span class='line'>systemctl <span class="nb">enable </span>pcsd.service
</span><span class='line'>
</span><span class='line'><span class="c"># create the mountpoint directory</span>
</span><span class='line'>mkdir -p /var/opt/opscode/drbd/data
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Run the following commands on the first cluster node only
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># authorize cluster</span>
</span><span class='line'>pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span><span class='line'>
</span><span class='line'><span class="c"># setup cluster</span>
</span><span class='line'>pcs cluster setup --start --name chef-ha backend0 backend1
</span><span class='line'>
</span><span class='line'><span class="c"># enable the cluster and examine status</span>
</span><span class='line'>pcs cluster <span class="nb">enable</span> --all
</span><span class='line'>pcs cluster status
</span><span class='line'>
</span><span class='line'><span class="c"># enable SCSI fence mode (uses SPC-3)</span>
</span><span class='line'>pcs stonith create scsi fence_scsi <span class="nv">devices</span><span class="o">=</span>/dev/sdb meta <span class="nv">provides</span><span class="o">=</span>unfencing
</span><span class='line'>sleep 5
</span><span class='line'><span class="c"># this might show stopped, try it a few times until it says started</span>
</span><span class='line'>pcs stonith show
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>And on the 2nd cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h2>Option 1: Leader Election and LVM Volume failover without CLVM</h2>

<ol>
<li>on the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>pcs status
</span><span class='line'>
</span><span class='line'><span class="c"># create the LVM PV and VG</span>
</span><span class='line'>pvcreate /dev/sdb
</span><span class='line'>vgcreate shared_vg /dev/sdb
</span><span class='line'>lvcreate -l 80%VG -n ha_lv shared_vg
</span><span class='line'>
</span><span class='line'><span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c">#   0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>lvchange -aey shared_vg
</span><span class='line'><span class="c">#   1 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'><span class="c"># format the volume</span>
</span><span class='line'>mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Update the initramfs device on all your cluster nodes, so that the CLVM volume is never auto-mounted:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>
</span><span class='line'><span class="c"># Set the lvm configuration to not auto-mount anything but the root &quot;centos&quot; VG</span>
</span><span class='line'><span class="c"># yes, the 783 part means the 783rd line of the file :\</span>
</span><span class='line'>sed -i.bak <span class="s2">&quot;783s/.*/    volume_list = [ \&quot;centos\&quot;, \&quot;@`hostname -s`\&quot; ]/&quot;</span> /etc/lvm/lvm.conf
</span><span class='line'><span class="c"># update initramfs and reboot</span>
</span><span class='line'>dracut -H -f /boot/initramfs-<span class="k">$(</span>uname -r<span class="k">)</span>.img <span class="k">$(</span>uname -r<span class="k">)</span>
</span><span class='line'><span class="c"># shutdown, but use vagrant to start them up</span>
</span><span class='line'>shutdown -h now
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>then bring both nodes back up
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant up backend0 backend1
</span></code></pre></td></tr></table></div></figure></li>
<li>Add an LVM resource
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># create resources for the LVM, filesystem and IP</span>
</span><span class='line'>pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> --group chef_ha
</span><span class='line'>pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="s2">&quot;/dev/shared_vg/ha_lv&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="s2">&quot;/var/opt/opscode/drbd/data&quot;</span> <span class="nv">fstype</span><span class="o">=</span><span class="s2">&quot;xfs&quot;</span> --group chef_ha
</span><span class='line'>pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> --group chef_ha
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'><span class="c"># debugging - only if things are going wrong</span>
</span><span class='line'><span class="c"># run lvs on both nodes to ensure it only says active (&quot;a&quot;) on backend0</span>
</span><span class='line'>lvs <span class="c"># on backend0</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-ao----  3.20g</span>
</span><span class='line'>lvs <span class="c"># on backend1</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-------  3.20g</span>
</span><span class='line'><span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span><span class='line'><span class="c"># or</span>
</span><span class='line'>pcs resource debug-start ha_lv
</span><span class='line'>pcs resource debug-start chef_data
</span><span class='line'>pcs resource debug-start backend_vip
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h2>Option 2:  Using CLVMD</h2>

<ol>
<li>On the first node: Enable clvmd and LVM clustering
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs resource create dlm ocf:pacemaker:controld clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs resource create clvmd ocf:heartbeat:clvm clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs constraint order start dlm-clone <span class="k">then</span> clvmd-clone
</span><span class='line'>pcs constraint colocation add clvmd-clone with dlm-clone
</span><span class='line'>
</span><span class='line'><span class="c"># enable LVM clustering with clvmd</span>
</span><span class='line'>lvmconf --enable-cluster
</span><span class='line'><span class="c"># stop lvmetad</span>
</span><span class='line'>killall lvmetad
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Run the following on the second cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>lvmconf --enable-cluster
</span><span class='line'>
</span><span class='line'><span class="c"># stop lvmetad</span>
</span><span class='line'>killall lvmetad
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Back to the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>pcs status
</span><span class='line'>
</span><span class='line'><span class="c"># create the LVM PV and VG</span>
</span><span class='line'>pvcreate /dev/sdb
</span><span class='line'>vgcreate -cy shared_vg /dev/sdb
</span><span class='line'>lvcreate -l 80%VG -n ha_lv shared_vg
</span><span class='line'>
</span><span class='line'><span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c"># 0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>vgchange -aey shared_vg
</span><span class='line'><span class="c"># 1 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'><span class="c"># run lvs on both nodes to ensure it only says active (&quot;a&quot;) on backend0</span>
</span><span class='line'>lvs <span class="c"># on backend0</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-a-----  3.20g</span>
</span><span class='line'>lvs <span class="c"># on backend1</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-------  3.20g</span>
</span><span class='line'>
</span><span class='line'><span class="c"># format the volume and mount it</span>
</span><span class='line'>mkfs.xfs /dev/shared_vg/ha_lv
</span></code></pre></td></tr></table></div></figure></li>
<li>Add an LVM resource (this is where shit gets whacky)
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># first unmount and deactivate</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c">#  0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'>pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> --group chef_ha
</span><span class='line'>pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="s2">&quot;/dev/shared_vg/ha_lv&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="s2">&quot;/var/opt/opscode/drbd/data&quot;</span> <span class="nv">fstype</span><span class="o">=</span><span class="s2">&quot;xfs&quot;</span> --group chef_ha
</span><span class='line'>pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> --group chef_ha
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c"># debugging</span>
</span><span class='line'>pcs resource debug-start ha_lv
</span><span class='line'>pcs resource debug-start chef_data
</span><span class='line'>pcs resource debug-start backend_vip
</span><span class='line'><span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<p>Now you&rsquo;re done!</p>

<h2>Failing over</h2>

<h3>the pacemaker way</h3>

<ol>
<li>bringing down backend0</li>
<li>Option 1:  shut down backend0 <code>shutdown -h now</code></li>
<li><p>Option 2:  make it a standby via Pacemaker <code>pcs cluster standby backend0</code></p></li>
<li><p>failover of resources should be automatic to the secondary node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 21:55:56 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Online: [ backend1 ]</span>
</span><span class='line'><span class="c"># OFFLINE: [ backend0 ]</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Full list of resources:</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started backend1</span>
</span><span class='line'><span class="c">#  Resource Group: chef_ha</span>
</span><span class='line'><span class="c">#      ha_lv  (ocf::heartbeat:LVM): Started backend1</span>
</span><span class='line'><span class="c">#      chef_data  (ocf::heartbeat:Filesystem):  Started backend1</span>
</span><span class='line'><span class="c">#      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># PCSD Status:</span>
</span><span class='line'><span class="c">#   backend0: Offline</span>
</span><span class='line'><span class="c">#   backend1: Online</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Daemon Status:</span>
</span><span class='line'><span class="c">#   corosync: active/enabled</span>
</span><span class='line'><span class="c">#   pacemaker: active/enabled</span>
</span><span class='line'><span class="c">#   pcsd: active/enabled</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li>Bringing backend0 back up and logging in, it won&rsquo;t want to run pacemaker
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status
</span><span class='line'><span class="c"># Error: cluster is not currently running on this node</span>
</span><span class='line'>systemctl start pacemaker
</span><span class='line'>
</span><span class='line'><span class="c"># now checking status</span>
</span><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 22:06:25 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Node backend0 (1): pending</span>
</span><span class='line'><span class="c"># Online: [ backend1 ]</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Full list of resources:</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started backend1</span>
</span><span class='line'><span class="c">#  Resource Group: chef_ha</span>
</span><span class='line'><span class="c">#      ha_lv  (ocf::heartbeat:LVM): Started backend1</span>
</span><span class='line'><span class="c">#      chef_data  (ocf::heartbeat:Filesystem):  Started backend1</span>
</span><span class='line'><span class="c">#      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># PCSD Status:</span>
</span><span class='line'><span class="c">#   backend0: Online</span>
</span><span class='line'><span class="c">#   backend1: Online</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Daemon Status:</span>
</span><span class='line'><span class="c">#   corosync: active/enabled</span>
</span><span class='line'><span class="c">#   pacemaker: active/enabled</span>
</span><span class='line'><span class="c">#   pcsd: active/enabled</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>If backend0 is considered standby, you can just unstandby it:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs cluster unstandby backend0
</span><span class='line'>
</span><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 22:11:53 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 22:11:52 2015 via crm_attribute on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Online: [ backend0 backend1 ]</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>What happens if you try to mount the disk on the inactive node?
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># LVM will be nice, but won&#39;t let you</span>
</span><span class='line'>vgchange -aey shared_vg
</span><span class='line'><span class="c"># 0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'>lvchange -aey shared_vg/ha_lv
</span><span class='line'>lvs
</span><span class='line'><span class="c"># LV    VG        Attr       LSize   Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c"># root  centos    -wi-ao----  38.48g</span>
</span><span class='line'><span class="c"># swap  centos    -wi-ao----   1.03g</span>
</span><span class='line'><span class="c"># ha_lv shared_vg -wi------- 816.00m</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h3>the CLVM way</h3>

<ol>
<li>On the active node
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># umount /var/opt/opscode/drbd/data</span>
</span><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># lvchange -an shared_vg/ha_lv</span>
</span><span class='line'>
</span><span class='line'><span class="c"># ensure the LV isn&#39;t active</span>
</span><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># lvs</span>
</span><span class='line'>  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
</span><span class='line'>  root  centos    -wi-ao---- 38.48g
</span><span class='line'>  swap  centos    -wi-ao----  1.03g
</span><span class='line'>  ha_lv shared_vg -wi-------  3.20g
</span></code></pre></td></tr></table></div></figure></li>
<li>on the standby node
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># lvs</span>
</span><span class='line'>  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
</span><span class='line'>  root  centos    -wi-ao---- 38.48g
</span><span class='line'>  swap  centos    -wi-ao----  1.03g
</span><span class='line'>  ha_lv shared_vg -wi-------  3.20g
</span><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># lvchange -aey shared_vg/ha_lv</span>
</span><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># mount /dev/shared_vg/ha_lv /var/opt/opscode/drbd/data</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h3>Forcing yourself to go standby</h3>

<p>TBD</p>

<h3>Forcing the other node to go standby (STONITH)</h3>

<p>TBD</p>

<h2>Troubleshooting</h2>

<h3>Do SCSI SPC-3 persistent reservations work on my device?</h3>

<p>good:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># exampling talking to the Linux iSCSI service:</span>
</span><span class='line'>sg_persist --in --report-capabilities -v /dev/sdb
</span><span class='line'><span class="c">#    inquiry cdb: 12 00 00 00 24 00</span>
</span><span class='line'><span class="c">#  LIO-ORG   block_backend     4.0</span>
</span><span class='line'><span class="c">#  Peripheral device type: disk</span>
</span><span class='line'><span class="c">#    Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</span>
</span><span class='line'><span class="c"># Report capabilities response:</span>
</span><span class='line'><span class="c">#  Compatible Reservation Handling(CRH): 1</span>
</span><span class='line'><span class="c">#  Specify Initiator Ports Capable(SIP_C): 1</span>
</span><span class='line'><span class="c">#  All Target Ports Capable(ATP_C): 1</span>
</span><span class='line'><span class="c">#  Persist Through Power Loss Capable(PTPL_C): 1</span>
</span><span class='line'><span class="c">#  Type Mask Valid(TMV): 1</span>
</span><span class='line'><span class="c">#  Allow Commands: 1</span>
</span><span class='line'><span class="c">#  Persist Through Power Loss Active(PTPL_A): 0</span>
</span><span class='line'><span class="c">#    Support indicated in Type mask:</span>
</span><span class='line'><span class="c">#      Write Exclusive, all registrants: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access, registrants only: 1</span>
</span><span class='line'><span class="c">#      Write Exclusive, registrants only: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access: 1</span>
</span><span class='line'><span class="c">#      Write Exclusive: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access, all registrants: 1</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>bad:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># example using Virtualbox SCSI/SAS/etc disks which are not SPC-compliant:</span>
</span><span class='line'>sg_persist --in --report-capabilities -v /dev/sdb
</span><span class='line'><span class="c">#    inquiry cdb: 12 00 00 00 24 00</span>
</span><span class='line'><span class="c">#  VBOX      HARDDISK          1.0</span>
</span><span class='line'><span class="c">#  Peripheral device type: disk</span>
</span><span class='line'><span class="c">#    Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</span>
</span><span class='line'><span class="c"># persistent reservation in:  Fixed format, current;  Sense key: Illegal Request</span>
</span><span class='line'><span class="c">#  Additional sense: Invalid command operation code</span>
</span><span class='line'><span class="c">#   Info fld=0x0 [0]</span>
</span><span class='line'><span class="c"># PR in (Report capabilities): command not supported</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>TBD</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World]]></title>
    <link href="http://irvingpop.github.io/blog/2015/01/01/hello-world/"/>
    <updated>2015-01-01T09:52:17-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/01/01/hello-world</id>
    <content type="html"><![CDATA[<h1>Hello, World!</h1>

<p>Daisy, Daisy, etc etc.</p>

<div class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">foo</span>
  <span class="nb">puts</span> <span class="s1">&#39;wat&#39;</span>
<span class="k">end</span></code></pre></div>


<p><img src="http://replygif.net/i/1463.gif"></p>

<!-- more -->


<p><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">blah</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s1">&#39;woohoo&#39;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>github gist:</p>

<div><script src='https://gist.github.com/3aee2f63a8fa7ac41f0a.js'></script>
<noscript><pre><code>{
  &quot;provider&quot;: &quot;ec2&quot;,
  &quot;ec2_options&quot;: {
    &quot;region&quot;: &quot;us-west-2&quot;,
    &quot;vpc_subnet&quot;: &quot;subnet-b2bb82f4&quot;,
    &quot;ami_id&quot;: &quot;ami-3d50120d&quot;,
    &quot;ssh_username&quot;: &quot;ubuntu&quot;,
    &quot;backend_storage_type&quot;: &quot;drbd&quot;,
    &quot;ebs_disk_size&quot;: &quot;100&quot;,
    &quot;use_private_ip_for_ssh&quot;: false,
    &quot;elb&quot;: true
  },
  &quot;default_package&quot;:   &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/trusty/chef-server-core_12.0.1-1_amd64.deb&quot;,
  &quot;manage_package&quot;:    &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/precise/opscode-manage_1.6.2-1_amd64.deb&quot;,
  &quot;reporting_package&quot;: &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/precise/opscode-reporting_1.2.2-1_amd64.deb&quot;,
  &quot;analytics_package&quot;: &quot;https://opscode-analytics-packages.s3.amazonaws.com/1.1.0-rc.4/ubuntu/14.04/x86_64/opscode-analytics_1.1.0-rc.4-1_amd64.deb?AWSAccessKeyId=AKIAI4DC2RTKK4T2T7JQ&amp;Expires=1419461962&amp;Signature=zvtEZJp6NVBqCOPAE%2BWtl6%2BN0L4%3D&quot;,
  &quot;apply_ec_bugfixes&quot;: false,
  &quot;lemme_doit&quot;: false,
  &quot;loadtesters&quot;: {
    &quot;num_loadtesters&quot;: 50,
    &quot;num_groups&quot;: 5,
    &quot;num_containers&quot;: 800
  },
  &quot;packages&quot;: {
  },
  &quot;layout&quot;: {
    &quot;topology&quot;: &quot;ha&quot;,
    &quot;api_fqdn&quot;: &quot;api.trusty.aws&quot;,
    &quot;manage_fqdn&quot;: &quot;manage.trusty.aws&quot;,
    &quot;analytics_fqdn&quot;: &quot;analytics.trusty.aws&quot;,
    &quot;configuration&quot;: {
      &quot;postgresql&quot;: {
          &quot;max_connections&quot;: 1500,
        &quot;log_min_duration_statement&quot;: 500
      },
      &quot;oc_id&quot;: {
        &quot;administrators&quot;: [&quot;pinkiepie&quot;, &quot;soarin&quot;]
      },
      &quot;opscode_erchef&quot;: {
          &quot;depsolver_worker_count&quot;: 4,
          &quot;depsolver_timeout&quot;: 120000,
          &quot;db_pool_size&quot;: 100
      },
      &quot;oc_bifrost&quot;: {
          &quot;db_pool_size&quot;: 100
      },
      &quot;opscode_certificate&quot;: {
          &quot;num_workers&quot;: 4,
          &quot;num_certificates_per_worker&quot;: 1000
      },
      &quot;oc_chef_authz&quot;: {
        &quot;http_init_count&quot;: 150,
        &quot;http_max_count&quot;: 150
      },
      &quot;nginx&quot;: {
        &quot;enable_non_ssl&quot;: true
      },
      &quot;license&quot;: {
        &quot;nodes&quot;: 100000
      }
    },
    &quot;backend_vip&quot;: {
      &quot;hostname&quot;: &quot;backend.trusty.aws&quot;,
      &quot;ipaddress&quot;: &quot;33.33.33.8&quot;,
      &quot;device&quot;: &quot;eth0&quot;,
      &quot;heartbeat_device&quot;: &quot;eth0&quot;
    },
    &quot;analytics_standalones&quot;: {
      &quot;dp-ub-analytics-standalone1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-analytics-standalone1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
        &quot;bootstrap&quot;: true
      }
    },
   &quot;frontends&quot;: {
      &quot;dp-ub-frontend1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend2&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend2.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend3&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend3.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend4&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend4.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      }
   },
   &quot;backends&quot;: {
      &quot;dp-ub-backend1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-backend1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
          &quot;bootstrap&quot;: true
      },
      &quot;dp-ub-backend2&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-backend2.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
          &quot;bootstrap&quot;: false
      }
    },
   &quot;loadtesters&quot;: {
      &quot;loadtester_spec&quot;: {
        &quot;hostname&quot;: &quot;loadtester1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.2xlarge&quot;
      }
    }
  }
}
</code></pre></noscript></div>



]]></content>
  </entry>
  
</feed>
