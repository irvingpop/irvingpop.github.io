<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Irving's blog]]></title>
  <link href="http://irvingpop.github.io/atom.xml" rel="self"/>
  <link href="http://irvingpop.github.io/"/>
  <updated>2015-04-10T13:48:02-07:00</updated>
  <id>http://irvingpop.github.io/</id>
  <author>
    <name><![CDATA[Irving Popovetsky]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Setting Up Your Private Supermarket Server]]></title>
    <link href="http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server/"/>
    <updated>2015-04-07T15:48:34-07:00</updated>
    <id>http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server</id>
    <content type="html"><![CDATA[<p><em>This is an updated version of the previous post from August, 2014: <a href="https://www.chef.io/blog/2014/08/29/getting-started-with-oc-id-and-supermarket/">Getting started with oc-id and Supermarket</a></em></p>

<p>Chef Server 12 includes <a href="https://github.com/chef/oc-id">oc-id</a>, the OAuth2 service that powers <a href="https://id.chef.io/">id.chef.io</a>.  After upgrading to this release, Chef customers can now run their own Supermarket service behind a firewall.</p>

<!-- more -->


<h2>oc-id setup on your Chef Server:</h2>

<p><em>You must be logged in to your Chef server via ssh and elevated to an admin user level for the following steps</em></p>

<ol>
<li><p>Add the following setting to your <code>/etc/opscode/chef-server.rb</code> configuration file:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="n">oc_id</span><span class="o">[</span><span class="s1">&#39;applications&#39;</span><span class="o">]</span> <span class="o">=</span> <span class="p">{</span>
</span><span class='line'>  <span class="s1">&#39;supermarket&#39;</span> <span class="o">=&gt;</span> <span class="p">{</span>
</span><span class='line'>    <span class="s1">&#39;redirect_uri&#39;</span> <span class="o">=&gt;</span> <span class="s1">&#39;https://supermarket.mycompany.com/auth/chef_oauth2/callback&#39;</span>
</span><span class='line'>  <span class="p">}</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>run <code>chef-server-ctl reconfigure</code></p></li>
<li><p>After the reconfigure, you will find the OAuth2 data in <code>/etc/opscode/oc-id-applications/supermarket.json</code>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='json'><span class='line'><span class="p">{</span>
</span><span class='line'>  <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;supermarket&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;uid&quot;</span><span class="p">:</span> <span class="s2">&quot;0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;secret&quot;</span><span class="p">:</span> <span class="s2">&quot;17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879&quot;</span><span class="p">,</span>
</span><span class='line'>  <span class="nt">&quot;redirect_uri&quot;</span><span class="p">:</span> <span class="s2">&quot;https://supermarket.mycompany.com/auth/chef_oauth2/callback&quot;</span>
</span><span class='line'><span class="p">}</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<p>Note the <code>uid</code> and <code>secret</code> values from this file, you will need them for the next stage.</p>

<p><em>You can add as many oc-id applications as you wish to the chef-server.rb configuration, it will create one file per application</em></p>

<h2>Running your Private Supermarket server in Test Kitchen</h2>

<p><em>Note: We will not use the community Supermarket cookbook, because at this time it installs Supermarket from source.  Instead, we will us an Omnibus package to install</em></p>

<p>In the spirit of &ldquo;code as documentation&rdquo; I&rsquo;ve provided a simple cookbook and test-kitchen configuration for testing Supermarket Omnibus packages. These packages are downloaded from <a href="https://packagecloud.io/chef/stable">https://packagecloud.io/chef/stable</a></p>

<ol>
<li><p>Download a copy of the <code>supermarket-omnibus-cookbook</code>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>git clone https://github.com/irvingpop/supermarket-omnibus-cookbook.git supermarket-omnibus-cookbook
</span><span class='line'><span class="nb">cd </span>supermarket-omnibus-cookbook
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Create a <code>.kitchen.local.yml</code> file, to set your oc-id attributes (as captured in step 3 above)
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">suites</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">default</span>
</span><span class='line'>    <span class="l-Scalar-Plain">run_list</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">recipe[supermarket-omnibus-cookbook::default]</span>
</span><span class='line'>    <span class="l-Scalar-Plain">attributes</span><span class="p-Indicator">:</span>
</span><span class='line'>      <span class="l-Scalar-Plain">supermarket_omnibus</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_server_url</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">https://chefserver.mycompany.com</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_app_id</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_secret</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879</span>
</span><span class='line'>        <span class="l-Scalar-Plain">chef_oauth2_verify_ssl</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">false</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Install the <code>vagrant-hostsupdater</code> plugin, this will automatically add the names of your machines to your /etc/hosts file. This is important for oauth2, which cares about host names. The <code>redirect_uri</code> value you entered in to your oc-id configuration reflects this name.
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant plugin install vagrant-hostsupdater
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Start your Supermarket instance and test it
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>kitchen converge default-centos-66 <span class="o">&amp;&amp;</span> kitchen verify default-centos-66
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>Go to your your Supermarket server and log in as a Chef user: <a href="https://default-centos-66">https://default-centos-66</a></p></li>
<li><p>Upon login, you should see:
<img src="https://www.getchef.com/blog/wp-content/uploads/2014/08/oc-id5-1024x343.png"></p></li>
</ol>


<h2>Troubleshooting &amp; FAQ</h2>

<h3>Incorrect redirect URL</h3>

<p>The redirect URL specified in oc-id <strong>MUST</strong> match the hostname of the Supermarket server. Also, you must get the URI correct (/auth/chef_oauth2/callback). If these are not true, you will recieve an error message like:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>The redirect uri included is not valid.
</span></code></pre></td></tr></table></div></figure></p>

<h3>Supermarket server cannot reach oc-id, throws 500 error during login</h3>

<p>The Supermarket server must be able to reach (via https) the specified <code>chef_server_url</code> - it does this during OAuth2 negotation. The most common problems are name resolution and firewall rules.</p>

<h3>Where can I find the code to Supermarket?</h3>

<ul>
<li>Supermarket the rails application is <a href="https://github.com/chef/supermarket">located here</a>

<ul>
<li>All Supermarket <a href="https://github.com/chef/supermarket/issues">issues should be reported there</a></li>
</ul>
</li>
<li>The code which builds Supermarket into an Omnibus package is <a href="https://github.com/chef/omnibus-supermarket">located here</a>

<ul>
<li>The cookbook that is run when during <code>supermarket-ctl reconfigure</code> is <a href="https://github.com/chef/omnibus-supermarket/tree/master/cookbooks/omnibus-supermarket">located within this repo</a></li>
<li>You can build your own Omnibus packages by following <a href="https://github.com/chef/omnibus-supermarket#kitchen-based-build-environment">the instructions in the README.md</a></li>
</ul>
</li>
</ul>


<h3>How do I enable rails application debug logging?</h3>

<p>There is a known issue with the Supermarket omnibus package that rails messages are not logged. To fix that requires a manual change at the moment. On your supermarket server, edit this file: <code>/opt/supermarket/embedded/service/supermarket/config/environments/production.rb</code>, change line 46 (<code>config.log_level = :warn</code>) to look like:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="no">Logger</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="s1">&#39;/var/log/supermarket/rails/rails.log&#39;</span><span class="p">)</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">level</span> <span class="o">=</span> <span class="s1">&#39;DEBUG&#39;</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">log_level</span> <span class="o">=</span> <span class="ss">:debug</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>Then restart the rails service by running
<code>supermarket-ctl restart rails</code></p>

<h3>Contacting packagecloud fails if I&rsquo;m behind a proxy</h3>

<p>No problem!  Add the following to your <code>.kitchenl.local.yml</code> file:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">provisioner</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">chef_zero</span>
</span><span class='line'>  <span class="l-Scalar-Plain">solo_rb</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">http_proxy</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">http://192.168.1.1</span>
</span><span class='line'>    <span class="l-Scalar-Plain">https_proxy</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">http://192.168.2.2</span>
</span></code></pre></td></tr></table></div></figure></p>

<h3>Test kitchen is slow because it has to download/install the Chef Omnibus client package every time</h3>

<p>Here&rsquo;s a few tips to speed it up:</p>

<ol>
<li>Tell test-kitchen to cache the Omnibus installer (put this in your <code>.kitchen.local.yml</code> file):
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="nn">---</span>
</span><span class='line'><span class="l-Scalar-Plain">provisioner</span><span class="p-Indicator">:</span>
</span><span class='line'>  <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">chef_zero</span>
</span><span class='line'>  <span class="l-Scalar-Plain">chef_omnibus_install_options</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">-d /tmp/vagrant-cache/vagrant_omnibus</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Cache yum repos like packagecloud using the vagrant-cachier plugin.  First run <code>vagrant plugin install vagrant-cachier</code>, then create a <code>$VAGRANT_HOME/Vagrantfile</code> that looks like so:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="no">Vagrant</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="s2">&quot;2&quot;</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>
</span><span class='line'>  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span> <span class="o">=</span> <span class="s1">&#39;some-box&#39;</span>
</span><span class='line'>  <span class="k">if</span> <span class="no">Vagrant</span><span class="o">.</span><span class="n">has_plugin?</span><span class="p">(</span><span class="s2">&quot;vagrant-cachier&quot;</span><span class="p">)</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">scope</span> <span class="o">=</span> <span class="ss">:box</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:chef</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:apt</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:yum</span>
</span><span class='line'>    <span class="n">config</span><span class="o">.</span><span class="n">cache</span><span class="o">.</span><span class="n">enable</span> <span class="ss">:gem</span>
</span><span class='line'>  <span class="k">end</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Experimenting With Redhat Cluster Suite 7]]></title>
    <link href="http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments/"/>
    <updated>2015-03-01T14:48:34-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments</id>
    <content type="html"><![CDATA[<h2>Description</h2>

<p>Source code at: <a href="https://github.com/irvingpop/rhcs7">https://github.com/irvingpop/rhcs7</a></p>

<p>I created a Vagrant-based configuration that brings up two CentOS 7 nodes that share a cluster disk with CLVM.
Based on:  <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html</a>
with clarification from: <a href="http://www.davidvossel.com/wiki/index.php?title=HA_LVM">http://www.davidvossel.com/wiki/index.php?title=HA_LVM</a></p>

<p>Ultimate goal: Find a way to couple Redhat clustering with Chef Server to create a more robust HA stack.</p>

<!-- more -->


<h2>Notes</h2>

<ul>
<li>Redhat clustering is HARD and not for the faint of heart.  It&rsquo;s not nearly as well documented as it should be, there&rsquo;s quite a few closely interconnected services and you should really take time out to understand it on its own before combining it with anything else.

<ul>
<li>Redhat clustering in EL5 and EL6 was pretty awful to operate and hard to understand.  It has been majorly redone in RHEL7 and is now considerably simpler to get going.</li>
</ul>
</li>
<li>This document and repo focus on the <em>shared storage</em> use case, where two nodes have access to the same block device but only one node can actively mount and write to it at a time. The simultaneous access use cases (GFS, OCFS, etc) are not covered here.</li>
<li>Initially I tried to use Virtualbox &ldquo;shareable&rdquo; block devices but this didn&rsquo;t work because it doesn&rsquo;t support the SCSI SPC-3 feature set.  This is important because the only applicable fence agent I could use is fence_scsi, which uses SCSI SPC-3 persistent reservations ( <a href="https://kb.netapp.com/support/index?page=content&amp;id=3012956">https://kb.netapp.com/support/index?page=content&amp;id=3012956</a> )</li>
<li>I switched to using the Linux iSCSI service on a separate node to get SPC-3.  In production you shouldn&rsquo;t use iSCSI.  It will burn your beans, melt your ice cream, and in general disappoint you continuously.  Yes, Fibre Channel is expensive and iSCSI is cheap, but over a decade of storage management experience has taught me that there&rsquo;s a good reason for that.</li>
</ul>


<p>There&rsquo;s two ways to do HA-LVM:
* Exclusive Activation via LVM volume_list filtering: This basically tells LVM not to auto-activate the shared storage, and then uses a tagging system (plus trust in pacemaker) to control exclusive access
* Exclusive Activation via clvmd: This uses the clvmd service (plus a distributed lock manager) to control access to LVM volume groups which are specially tagged as &ldquo;clustered&rdquo;
* Irving&rsquo;s thoughts: clvmd would appear to be the safer way to go because it protects against &ldquo;rogue&rdquo; nodes, but in practice clvmd is really freakin hard to figure out and troubleshoot. Also, the SCSI SPC-3 persistent reservations provide an additional layer of safety.  So I recommend Option 1, the volume_list filtering approach as it gave me far less heartburn in testing.</p>

<h2>Initial setup</h2>

<ol>
<li>Initial configuration of the cluster machines, backend0 and backend1:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant up
</span></code></pre></td></tr></table></div></figure></li>
<li><p>iSCSI server setup
from: <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup</a>
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install</span>
</span><span class='line'>yum install -y targetcli
</span><span class='line'><span class="c"># start and enable the service</span>
</span><span class='line'>systemctl start target <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable </span>target
</span><span class='line'>
</span><span class='line'><span class="c"># map the backstore to a physical unformatted block device</span>
</span><span class='line'>targetcli /backstores/block create <span class="nv">name</span><span class="o">=</span>block_backend <span class="nv">dev</span><span class="o">=</span>/dev/sdb
</span><span class='line'>
</span><span class='line'><span class="c"># create an iSCSI target</span>
</span><span class='line'>targetcli /iscsi create iqn.2006-04.com.iscsi-is-awesome:1
</span><span class='line'><span class="c">#  Created target iqn.2006-04.com.iscsi-is-awesome:1</span>
</span><span class='line'><span class="c">#  Created TPG 1.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># create the iSCSI portal (IP listener)</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/portals/ create
</span><span class='line'><span class="c"># Using default IP port 3260</span>
</span><span class='line'><span class="c"># Binding to INADDR_ANY (0.0.0.0)</span>
</span><span class='line'><span class="c"># Created network portal 0.0.0.0:3260.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># Map the iSCSI target to the backstore</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/luns/ create /backstores/block/block_backend
</span><span class='line'><span class="c"># Created LUN 0.</span>
</span><span class='line'>
</span><span class='line'><span class="c"># crazy wild-west no-ACL mode, because this is a PoC :)</span>
</span><span class='line'>targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/ <span class="nb">set </span>attribute <span class="nv">authentication</span><span class="o">=</span><span class="m">0</span> <span class="nv">demo_mode_write_protect</span><span class="o">=</span><span class="m">0</span> <span class="nv">generate_node_acls</span><span class="o">=</span><span class="m">1</span> <span class="nv">cache_dynamic_acls</span><span class="o">=</span>1
</span><span class='line'><span class="c"># Parameter demo_mode_write_protect is now &#39;0&#39;.</span>
</span><span class='line'><span class="c"># Parameter authentication is now &#39;0&#39;.</span>
</span><span class='line'><span class="c"># Parameter generate_node_acls is now &#39;1&#39;.</span>
</span><span class='line'><span class="c"># Parameter cache_dynamic_acls is now &#39;1&#39;.</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c"># sit back and marvel at your iSCSIs</span>
</span><span class='line'>targetcli ls
</span><span class='line'><span class="c"># o- / ........................................ [...]</span>
</span><span class='line'><span class="c">#  o- backstores ............................. [...]</span>
</span><span class='line'><span class="c">#  | o- block ................. [Storage Objects: 1]</span>
</span><span class='line'><span class="c">#  | | o- block_backend  [/dev/sdb (1.0GiB) write-thru activated]</span>
</span><span class='line'><span class="c">#  | o- fileio ................ [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  | o- pscsi ................. [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  | o- ramdisk ............... [Storage Objects: 0]</span>
</span><span class='line'><span class="c">#  o- iscsi ........................... [Targets: 1]</span>
</span><span class='line'><span class="c">#  | o- iqn.2006-04.com.iscsi-is-awesome:1  [TPGs: 1]</span>
</span><span class='line'><span class="c">#  |   o- tpg1 .............. [no-gen-acls, no-auth]</span>
</span><span class='line'><span class="c">#  |     o- acls ......................... [ACLs: 0]</span>
</span><span class='line'><span class="c">#  |     o- luns ......................... [LUNs: 1]</span>
</span><span class='line'><span class="c">#  |     | o- lun0  [block/block_backend (/dev/sdb)]</span>
</span><span class='line'><span class="c">#  |     o- portals ................... [Portals: 1]</span>
</span><span class='line'><span class="c">#  |       o- 0.0.0.0:3260 .................... [OK]</span>
</span><span class='line'><span class="c">#  o- loopback ........................ [Targets: 0]</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li><p>iSCSI client setup
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install and start the iscsi client service</span>
</span><span class='line'>yum install -y iscsi-initiator-utils
</span><span class='line'>systemctl start iscsid.service <span class="o">&amp;&amp;</span> systemctl <span class="nb">enable </span>iscsid.service
</span><span class='line'>
</span><span class='line'><span class="c"># connect to the iscsi server</span>
</span><span class='line'>iscsiadm -m node -o new -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260
</span><span class='line'>
</span><span class='line'><span class="c"># login</span>
</span><span class='line'>iscsiadm -m node -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260 --login
</span><span class='line'><span class="c"># Logging in to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] (multiple)</span>
</span><span class='line'><span class="c"># Login to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] successful.</span>
</span></code></pre></td></tr></table></div></figure></p></li>
</ol>


<h2>Base Cluster Setup</h2>

<ol>
<li>Run the following commands on both nodes
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># install clustering packages</span>
</span><span class='line'>yum -y install pcs fence-agents-all lvm2-cluster
</span><span class='line'>
</span><span class='line'><span class="c"># set hacluster user password</span>
</span><span class='line'><span class="nb">echo</span> <span class="s2">&quot;hacluster&quot;</span> <span class="p">|</span> passwd hacluster --stdin
</span><span class='line'>
</span><span class='line'><span class="c"># start clustering services</span>
</span><span class='line'>systemctl start pcsd.service
</span><span class='line'>systemctl <span class="nb">enable </span>pcsd.service
</span><span class='line'>
</span><span class='line'><span class="c"># create the mountpoint directory</span>
</span><span class='line'>mkdir -p /var/opt/opscode/drbd/data
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Run the following commands on the first cluster node only
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># authorize cluster</span>
</span><span class='line'>pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span><span class='line'>
</span><span class='line'><span class="c"># setup cluster</span>
</span><span class='line'>pcs cluster setup --start --name chef-ha backend0 backend1
</span><span class='line'>
</span><span class='line'><span class="c"># enable the cluster and examine status</span>
</span><span class='line'>pcs cluster <span class="nb">enable</span> --all
</span><span class='line'>pcs cluster status
</span><span class='line'>
</span><span class='line'><span class="c"># enable SCSI fence mode (uses SPC-3)</span>
</span><span class='line'>pcs stonith create scsi fence_scsi <span class="nv">devices</span><span class="o">=</span>/dev/sdb meta <span class="nv">provides</span><span class="o">=</span>unfencing
</span><span class='line'>sleep 5
</span><span class='line'><span class="c"># this might show stopped, try it a few times until it says started</span>
</span><span class='line'>pcs stonith show
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>And on the 2nd cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h2>Option 1: Leader Election and LVM Volume failover without CLVM</h2>

<ol>
<li>on the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>pcs status
</span><span class='line'>
</span><span class='line'><span class="c"># create the LVM PV and VG</span>
</span><span class='line'>pvcreate /dev/sdb
</span><span class='line'>vgcreate shared_vg /dev/sdb
</span><span class='line'>lvcreate -l 80%VG -n ha_lv shared_vg
</span><span class='line'>
</span><span class='line'><span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c">#   0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>lvchange -aey shared_vg
</span><span class='line'><span class="c">#   1 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'><span class="c"># format the volume</span>
</span><span class='line'>mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Update the initramfs device on all your cluster nodes, so that the CLVM volume is never auto-mounted:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>
</span><span class='line'><span class="c"># Set the lvm configuration to not auto-mount anything but the root &quot;centos&quot; VG</span>
</span><span class='line'><span class="c"># yes, the 783 part means the 783rd line of the file :\</span>
</span><span class='line'>sed -i.bak <span class="s2">&quot;783s/.*/    volume_list = [ \&quot;centos\&quot;, \&quot;@`hostname -s`\&quot; ]/&quot;</span> /etc/lvm/lvm.conf
</span><span class='line'><span class="c"># update initramfs and reboot</span>
</span><span class='line'>dracut -H -f /boot/initramfs-<span class="k">$(</span>uname -r<span class="k">)</span>.img <span class="k">$(</span>uname -r<span class="k">)</span>
</span><span class='line'><span class="c"># shutdown, but use vagrant to start them up</span>
</span><span class='line'>shutdown -h now
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>then bring both nodes back up
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>vagrant up backend0 backend1
</span></code></pre></td></tr></table></div></figure></li>
<li>Add an LVM resource
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># create resources for the LVM, filesystem and IP</span>
</span><span class='line'>pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> --group chef_ha
</span><span class='line'>pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="s2">&quot;/dev/shared_vg/ha_lv&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="s2">&quot;/var/opt/opscode/drbd/data&quot;</span> <span class="nv">fstype</span><span class="o">=</span><span class="s2">&quot;xfs&quot;</span> --group chef_ha
</span><span class='line'>pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> --group chef_ha
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'><span class="c"># debugging - only if things are going wrong</span>
</span><span class='line'><span class="c"># run lvs on both nodes to ensure it only says active (&quot;a&quot;) on backend0</span>
</span><span class='line'>lvs <span class="c"># on backend0</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-ao----  3.20g</span>
</span><span class='line'>lvs <span class="c"># on backend1</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-------  3.20g</span>
</span><span class='line'><span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span><span class='line'><span class="c"># or</span>
</span><span class='line'>pcs resource debug-start ha_lv
</span><span class='line'>pcs resource debug-start chef_data
</span><span class='line'>pcs resource debug-start backend_vip
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h2>Option 2:  Using CLVMD</h2>

<ol>
<li>On the first node: Enable clvmd and LVM clustering
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs resource create dlm ocf:pacemaker:controld clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs resource create clvmd ocf:heartbeat:clvm clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs constraint order start dlm-clone <span class="k">then</span> clvmd-clone
</span><span class='line'>pcs constraint colocation add clvmd-clone with dlm-clone
</span><span class='line'>
</span><span class='line'><span class="c"># enable LVM clustering with clvmd</span>
</span><span class='line'>lvmconf --enable-cluster
</span><span class='line'><span class="c"># stop lvmetad</span>
</span><span class='line'>killall lvmetad
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Run the following on the second cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>lvmconf --enable-cluster
</span><span class='line'>
</span><span class='line'><span class="c"># stop lvmetad</span>
</span><span class='line'>killall lvmetad
</span><span class='line'><span class="c">#</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>Back to the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>pcs status
</span><span class='line'>
</span><span class='line'><span class="c"># create the LVM PV and VG</span>
</span><span class='line'>pvcreate /dev/sdb
</span><span class='line'>vgcreate -cy shared_vg /dev/sdb
</span><span class='line'>lvcreate -l 80%VG -n ha_lv shared_vg
</span><span class='line'>
</span><span class='line'><span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c"># 0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>vgchange -aey shared_vg
</span><span class='line'><span class="c"># 1 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'><span class="c"># run lvs on both nodes to ensure it only says active (&quot;a&quot;) on backend0</span>
</span><span class='line'>lvs <span class="c"># on backend0</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-a-----  3.20g</span>
</span><span class='line'>lvs <span class="c"># on backend1</span>
</span><span class='line'><span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c">#  root  centos    -wi-ao---- 38.48g</span>
</span><span class='line'><span class="c">#  swap  centos    -wi-ao----  1.03g</span>
</span><span class='line'><span class="c">#  ha_lv shared_vg -wi-------  3.20g</span>
</span><span class='line'>
</span><span class='line'><span class="c"># format the volume and mount it</span>
</span><span class='line'>mkfs.xfs /dev/shared_vg/ha_lv
</span></code></pre></td></tr></table></div></figure></li>
<li>Add an LVM resource (this is where shit gets whacky)
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># first unmount and deactivate</span>
</span><span class='line'>vgchange -an shared_vg
</span><span class='line'><span class="c">#  0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'>pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> --group chef_ha
</span><span class='line'>pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="s2">&quot;/dev/shared_vg/ha_lv&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="s2">&quot;/var/opt/opscode/drbd/data&quot;</span> <span class="nv">fstype</span><span class="o">=</span><span class="s2">&quot;xfs&quot;</span> --group chef_ha
</span><span class='line'>pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> --group chef_ha
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="c"># debugging</span>
</span><span class='line'>pcs resource debug-start ha_lv
</span><span class='line'>pcs resource debug-start chef_data
</span><span class='line'>pcs resource debug-start backend_vip
</span><span class='line'><span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<p>Now you&rsquo;re done!</p>

<h2>Failing over</h2>

<h3>the pacemaker way</h3>

<ol>
<li>bringing down backend0</li>
<li>Option 1:  shut down backend0 <code>shutdown -h now</code></li>
<li><p>Option 2:  make it a standby via Pacemaker <code>pcs cluster standby backend0</code></p></li>
<li><p>failover of resources should be automatic to the secondary node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 21:55:56 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Online: [ backend1 ]</span>
</span><span class='line'><span class="c"># OFFLINE: [ backend0 ]</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Full list of resources:</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started backend1</span>
</span><span class='line'><span class="c">#  Resource Group: chef_ha</span>
</span><span class='line'><span class="c">#      ha_lv  (ocf::heartbeat:LVM): Started backend1</span>
</span><span class='line'><span class="c">#      chef_data  (ocf::heartbeat:Filesystem):  Started backend1</span>
</span><span class='line'><span class="c">#      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># PCSD Status:</span>
</span><span class='line'><span class="c">#   backend0: Offline</span>
</span><span class='line'><span class="c">#   backend1: Online</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Daemon Status:</span>
</span><span class='line'><span class="c">#   corosync: active/enabled</span>
</span><span class='line'><span class="c">#   pacemaker: active/enabled</span>
</span><span class='line'><span class="c">#   pcsd: active/enabled</span>
</span></code></pre></td></tr></table></div></figure></p></li>
<li>Bringing backend0 back up and logging in, it won&rsquo;t want to run pacemaker
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status
</span><span class='line'><span class="c"># Error: cluster is not currently running on this node</span>
</span><span class='line'>systemctl start pacemaker
</span><span class='line'>
</span><span class='line'><span class="c"># now checking status</span>
</span><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 22:06:25 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Node backend0 (1): pending</span>
</span><span class='line'><span class="c"># Online: [ backend1 ]</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Full list of resources:</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#  scsi (stonith:fence_scsi): Started backend1</span>
</span><span class='line'><span class="c">#  Resource Group: chef_ha</span>
</span><span class='line'><span class="c">#      ha_lv  (ocf::heartbeat:LVM): Started backend1</span>
</span><span class='line'><span class="c">#      chef_data  (ocf::heartbeat:Filesystem):  Started backend1</span>
</span><span class='line'><span class="c">#      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># PCSD Status:</span>
</span><span class='line'><span class="c">#   backend0: Online</span>
</span><span class='line'><span class="c">#   backend1: Online</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Daemon Status:</span>
</span><span class='line'><span class="c">#   corosync: active/enabled</span>
</span><span class='line'><span class="c">#   pacemaker: active/enabled</span>
</span><span class='line'><span class="c">#   pcsd: active/enabled</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>If backend0 is considered standby, you can just unstandby it:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs cluster unstandby backend0
</span><span class='line'>
</span><span class='line'>pcs status
</span><span class='line'><span class="c"># Cluster name: chef-ha</span>
</span><span class='line'><span class="c"># Last updated: Sat Mar 14 22:11:53 2015</span>
</span><span class='line'><span class="c"># Last change: Sat Mar 14 22:11:52 2015 via crm_attribute on backend0</span>
</span><span class='line'><span class="c"># Stack: corosync</span>
</span><span class='line'><span class="c"># Current DC: backend1 (2) - partition with quorum</span>
</span><span class='line'><span class="c"># Version: 1.1.10-32.el7_0.1-368c726</span>
</span><span class='line'><span class="c"># 2 Nodes configured</span>
</span><span class='line'><span class="c"># 4 Resources configured</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c"># Online: [ backend0 backend1 ]</span>
</span></code></pre></td></tr></table></div></figure></li>
<li>What happens if you try to mount the disk on the inactive node?
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># LVM will be nice, but won&#39;t let you</span>
</span><span class='line'>vgchange -aey shared_vg
</span><span class='line'><span class="c"># 0 logical volume(s) in volume group &quot;shared_vg&quot; now active</span>
</span><span class='line'>
</span><span class='line'>lvchange -aey shared_vg/ha_lv
</span><span class='line'>lvs
</span><span class='line'><span class="c"># LV    VG        Attr       LSize   Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'><span class="c"># root  centos    -wi-ao----  38.48g</span>
</span><span class='line'><span class="c"># swap  centos    -wi-ao----   1.03g</span>
</span><span class='line'><span class="c"># ha_lv shared_vg -wi------- 816.00m</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h3>the CLVM way</h3>

<ol>
<li>On the active node
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># umount /var/opt/opscode/drbd/data</span>
</span><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># lvchange -an shared_vg/ha_lv</span>
</span><span class='line'>
</span><span class='line'><span class="c"># ensure the LV isn&#39;t active</span>
</span><span class='line'><span class="o">[</span>root@backend0 ~<span class="o">]</span><span class="c"># lvs</span>
</span><span class='line'>  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
</span><span class='line'>  root  centos    -wi-ao---- 38.48g
</span><span class='line'>  swap  centos    -wi-ao----  1.03g
</span><span class='line'>  ha_lv shared_vg -wi-------  3.20g
</span></code></pre></td></tr></table></div></figure></li>
<li>on the standby node
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># lvs</span>
</span><span class='line'>  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
</span><span class='line'>  root  centos    -wi-ao---- 38.48g
</span><span class='line'>  swap  centos    -wi-ao----  1.03g
</span><span class='line'>  ha_lv shared_vg -wi-------  3.20g
</span><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># lvchange -aey shared_vg/ha_lv</span>
</span><span class='line'><span class="o">[</span>root@backend1 ~<span class="o">]</span><span class="c"># mount /dev/shared_vg/ha_lv /var/opt/opscode/drbd/data</span>
</span></code></pre></td></tr></table></div></figure></li>
</ol>


<h3>Forcing yourself to go standby</h3>

<p>TBD</p>

<h3>Forcing the other node to go standby (STONITH)</h3>

<p>TBD</p>

<h2>Troubleshooting</h2>

<h3>Do SCSI SPC-3 persistent reservations work on my device?</h3>

<p>good:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># exampling talking to the Linux iSCSI service:</span>
</span><span class='line'>sg_persist --in --report-capabilities -v /dev/sdb
</span><span class='line'><span class="c">#    inquiry cdb: 12 00 00 00 24 00</span>
</span><span class='line'><span class="c">#  LIO-ORG   block_backend     4.0</span>
</span><span class='line'><span class="c">#  Peripheral device type: disk</span>
</span><span class='line'><span class="c">#    Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</span>
</span><span class='line'><span class="c"># Report capabilities response:</span>
</span><span class='line'><span class="c">#  Compatible Reservation Handling(CRH): 1</span>
</span><span class='line'><span class="c">#  Specify Initiator Ports Capable(SIP_C): 1</span>
</span><span class='line'><span class="c">#  All Target Ports Capable(ATP_C): 1</span>
</span><span class='line'><span class="c">#  Persist Through Power Loss Capable(PTPL_C): 1</span>
</span><span class='line'><span class="c">#  Type Mask Valid(TMV): 1</span>
</span><span class='line'><span class="c">#  Allow Commands: 1</span>
</span><span class='line'><span class="c">#  Persist Through Power Loss Active(PTPL_A): 0</span>
</span><span class='line'><span class="c">#    Support indicated in Type mask:</span>
</span><span class='line'><span class="c">#      Write Exclusive, all registrants: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access, registrants only: 1</span>
</span><span class='line'><span class="c">#      Write Exclusive, registrants only: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access: 1</span>
</span><span class='line'><span class="c">#      Write Exclusive: 1</span>
</span><span class='line'><span class="c">#      Exclusive Access, all registrants: 1</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>bad:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'><span class="c"># example using Virtualbox SCSI/SAS/etc disks which are not SPC-compliant:</span>
</span><span class='line'>sg_persist --in --report-capabilities -v /dev/sdb
</span><span class='line'><span class="c">#    inquiry cdb: 12 00 00 00 24 00</span>
</span><span class='line'><span class="c">#  VBOX      HARDDISK          1.0</span>
</span><span class='line'><span class="c">#  Peripheral device type: disk</span>
</span><span class='line'><span class="c">#    Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</span>
</span><span class='line'><span class="c"># persistent reservation in:  Fixed format, current;  Sense key: Illegal Request</span>
</span><span class='line'><span class="c">#  Additional sense: Invalid command operation code</span>
</span><span class='line'><span class="c">#   Info fld=0x0 [0]</span>
</span><span class='line'><span class="c"># PR in (Report capabilities): command not supported</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>TBD</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Hello World]]></title>
    <link href="http://irvingpop.github.io/blog/2015/01/01/hello-world/"/>
    <updated>2015-01-01T09:52:17-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/01/01/hello-world</id>
    <content type="html"><![CDATA[<h1>Hello, World!</h1>

<p>Daisy, Daisy, etc etc.</p>

<div class="highlight"><pre><code class="language-ruby" data-lang="ruby"><span class="k">def</span> <span class="nf">foo</span>
  <span class="nb">puts</span> <span class="s1">&#39;wat&#39;</span>
<span class="k">end</span></code></pre></div>


<p><img src="http://replygif.net/i/1463.gif"></p>

<!-- more -->


<p><figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
</pre></td><td class='code'><pre><code class='ruby'><span class='line'><span class="k">def</span> <span class="nf">blah</span>
</span><span class='line'>  <span class="nb">puts</span> <span class="s1">&#39;woohoo&#39;</span>
</span><span class='line'><span class="k">end</span>
</span></code></pre></td></tr></table></div></figure></p>

<p>github gist:</p>

<div><script src='https://gist.github.com/3aee2f63a8fa7ac41f0a.js'></script>
<noscript><pre><code>{
  &quot;provider&quot;: &quot;ec2&quot;,
  &quot;ec2_options&quot;: {
    &quot;region&quot;: &quot;us-west-2&quot;,
    &quot;vpc_subnet&quot;: &quot;subnet-b2bb82f4&quot;,
    &quot;ami_id&quot;: &quot;ami-3d50120d&quot;,
    &quot;ssh_username&quot;: &quot;ubuntu&quot;,
    &quot;backend_storage_type&quot;: &quot;drbd&quot;,
    &quot;ebs_disk_size&quot;: &quot;100&quot;,
    &quot;use_private_ip_for_ssh&quot;: false,
    &quot;elb&quot;: true
  },
  &quot;default_package&quot;:   &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/trusty/chef-server-core_12.0.1-1_amd64.deb&quot;,
  &quot;manage_package&quot;:    &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/precise/opscode-manage_1.6.2-1_amd64.deb&quot;,
  &quot;reporting_package&quot;: &quot;https://web-dl.packagecloud.io/chef/stable/packages/ubuntu/precise/opscode-reporting_1.2.2-1_amd64.deb&quot;,
  &quot;analytics_package&quot;: &quot;https://opscode-analytics-packages.s3.amazonaws.com/1.1.0-rc.4/ubuntu/14.04/x86_64/opscode-analytics_1.1.0-rc.4-1_amd64.deb?AWSAccessKeyId=AKIAI4DC2RTKK4T2T7JQ&amp;Expires=1419461962&amp;Signature=zvtEZJp6NVBqCOPAE%2BWtl6%2BN0L4%3D&quot;,
  &quot;apply_ec_bugfixes&quot;: false,
  &quot;lemme_doit&quot;: false,
  &quot;loadtesters&quot;: {
    &quot;num_loadtesters&quot;: 50,
    &quot;num_groups&quot;: 5,
    &quot;num_containers&quot;: 800
  },
  &quot;packages&quot;: {
  },
  &quot;layout&quot;: {
    &quot;topology&quot;: &quot;ha&quot;,
    &quot;api_fqdn&quot;: &quot;api.trusty.aws&quot;,
    &quot;manage_fqdn&quot;: &quot;manage.trusty.aws&quot;,
    &quot;analytics_fqdn&quot;: &quot;analytics.trusty.aws&quot;,
    &quot;configuration&quot;: {
      &quot;postgresql&quot;: {
          &quot;max_connections&quot;: 1500,
        &quot;log_min_duration_statement&quot;: 500
      },
      &quot;oc_id&quot;: {
        &quot;administrators&quot;: [&quot;pinkiepie&quot;, &quot;soarin&quot;]
      },
      &quot;opscode_erchef&quot;: {
          &quot;depsolver_worker_count&quot;: 4,
          &quot;depsolver_timeout&quot;: 120000,
          &quot;db_pool_size&quot;: 100
      },
      &quot;oc_bifrost&quot;: {
          &quot;db_pool_size&quot;: 100
      },
      &quot;opscode_certificate&quot;: {
          &quot;num_workers&quot;: 4,
          &quot;num_certificates_per_worker&quot;: 1000
      },
      &quot;oc_chef_authz&quot;: {
        &quot;http_init_count&quot;: 150,
        &quot;http_max_count&quot;: 150
      },
      &quot;nginx&quot;: {
        &quot;enable_non_ssl&quot;: true
      },
      &quot;license&quot;: {
        &quot;nodes&quot;: 100000
      }
    },
    &quot;backend_vip&quot;: {
      &quot;hostname&quot;: &quot;backend.trusty.aws&quot;,
      &quot;ipaddress&quot;: &quot;33.33.33.8&quot;,
      &quot;device&quot;: &quot;eth0&quot;,
      &quot;heartbeat_device&quot;: &quot;eth0&quot;
    },
    &quot;analytics_standalones&quot;: {
      &quot;dp-ub-analytics-standalone1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-analytics-standalone1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
        &quot;bootstrap&quot;: true
      }
    },
   &quot;frontends&quot;: {
      &quot;dp-ub-frontend1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend2&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend2.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend3&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend3.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      },
      &quot;dp-ub-frontend4&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-frontend4.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;
      }
   },
   &quot;backends&quot;: {
      &quot;dp-ub-backend1&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-backend1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
          &quot;bootstrap&quot;: true
      },
      &quot;dp-ub-backend2&quot;: {
        &quot;hostname&quot;: &quot;dp-ub-backend2.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.xlarge&quot;,
          &quot;bootstrap&quot;: false
      }
    },
   &quot;loadtesters&quot;: {
      &quot;loadtester_spec&quot;: {
        &quot;hostname&quot;: &quot;loadtester1.trusty.aws&quot;,
        &quot;ebs_optimized&quot;: true,
        &quot;instance_type&quot;: &quot;m3.2xlarge&quot;
      }
    }
  }
}
</code></pre></noscript></div>



]]></content>
  </entry>
  
</feed>
