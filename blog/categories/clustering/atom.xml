<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Clustering | Irving's blog]]></title>
  <link href="http://irvingpop.github.io/blog/categories/clustering/atom.xml" rel="self"/>
  <link href="http://irvingpop.github.io/"/>
  <updated>2015-04-20T14:07:37-07:00</updated>
  <id>http://irvingpop.github.io/</id>
  <author>
    <name><![CDATA[Irving Popovetsky]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Experimenting With Redhat Cluster Suite 7]]></title>
    <link href="http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments/"/>
    <updated>2015-03-01T14:48:34-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments</id>
    <content type="html"><![CDATA[<h2>Description</h2>

<p>Source code at: <a href="https://github.com/irvingpop/rhcs7">https://github.com/irvingpop/rhcs7</a></p>

<p>I created a Vagrant-based configuration that brings up two CentOS 7 nodes that share a cluster disk with CLVM.
Based on:  <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html</a>
with clarification from: <a href="http://www.davidvossel.com/wiki/index.php?title=HA_LVM">http://www.davidvossel.com/wiki/index.php?title=HA_LVM</a></p>

<p>Ultimate goal: Find a way to couple Redhat clustering with Chef Server to create a more robust HA stack.</p>

<!-- more -->


<h2>Notes</h2>

<ul>
<li>Redhat clustering is HARD and not for the faint of heart.  It&rsquo;s not nearly as well documented as it should be, there&rsquo;s quite a few closely interconnected services and you should really take time out to understand it on its own before combining it with anything else.

<ul>
<li>Redhat clustering in EL5 and EL6 was pretty awful to operate and hard to understand.  It has been majorly redone in RHEL7 and is now considerably simpler to get going.</li>
</ul>
</li>
<li>This document and repo focus on the <em>shared storage</em> use case, where two nodes have access to the same block device but only one node can actively mount and write to it at a time. The simultaneous access use cases (GFS, OCFS, etc) are not covered here.</li>
<li>Initially I tried to use Virtualbox &ldquo;shareable&rdquo; block devices but this didn&rsquo;t work because it doesn&rsquo;t support the SCSI SPC-3 feature set.  This is important because the only applicable fence agent I could use is fence_scsi, which uses SCSI SPC-3 persistent reservations ( <a href="https://kb.netapp.com/support/index?page=content&amp;id=3012956">https://kb.netapp.com/support/index?page=content&amp;id=3012956</a> )</li>
<li>I switched to using the Linux iSCSI service on a separate node to get SPC-3.  In production you shouldn&rsquo;t use iSCSI.  It will burn your beans, melt your ice cream, and in general disappoint you continuously.  Yes, Fibre Channel is expensive and iSCSI is cheap, but over a decade of storage management experience has taught me that there&rsquo;s a good reason for that.</li>
</ul>


<p>There&rsquo;s two ways to do HA-LVM:
* Exclusive Activation via LVM volume_list filtering: This basically tells LVM not to auto-activate the shared storage, and then uses a tagging system (plus trust in pacemaker) to control exclusive access
* Exclusive Activation via clvmd: This uses the clvmd service (plus a distributed lock manager) to control access to LVM volume groups which are specially tagged as &ldquo;clustered&rdquo;
* Irving&rsquo;s thoughts: clvmd would appear to be the safer way to go because it protects against &ldquo;rogue&rdquo; nodes, but in practice clvmd is really freakin hard to figure out and troubleshoot. Also, the SCSI SPC-3 persistent reservations provide an additional layer of safety.  So I recommend Option 1, the volume_list filtering approach as it gave me far less heartburn in testing.</p>

<h2>Initial setup</h2>

<ol>
<li>Initial configuration of the cluster machines, backend0 and backend1:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  vagrant up
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>1. iSCSI server setup
</span><span class='line'>from: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># install</span>
</span><span class='line'>  yum install -y targetcli
</span><span class='line'>  <span class="c"># start and enable the service</span>
</span><span class='line'>  systemctl start target <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl <span class="nb">enable </span>target&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># map the backstore to a physical unformatted block device</span>
</span><span class='line'>  targetcli /backstores/block create <span class="nv">name</span><span class="o">=</span>block_backend <span class="nv">dev</span><span class="o">=</span>/dev/sdb&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create an iSCSI target</span>
</span><span class='line'>  targetcli /iscsi create iqn.2006-04.com.iscsi-is-awesome:1
</span><span class='line'>  <span class="c">#  Created target iqn.2006-04.com.iscsi-is-awesome:1</span>
</span><span class='line'>  <span class="c">#  Created TPG 1.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the iSCSI portal (IP listener)</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/portals/ create
</span><span class='line'>  <span class="c"># Using default IP port 3260</span>
</span><span class='line'>  <span class="c"># Binding to INADDR_ANY (0.0.0.0)</span>
</span><span class='line'>  <span class="c"># Created network portal 0.0.0.0:3260.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># Map the iSCSI target to the backstore</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/luns/ create /backstores/block/block_backend
</span><span class='line'>  <span class="c"># Created LUN 0.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># crazy wild-west no-ACL mode, because this is a PoC :)</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/ <span class="nb">set </span>attribute <span class="nv">authentication</span><span class="o">=</span><span class="m">0</span> <span class="nv">demo_mode_write_protect</span><span class="o">=</span><span class="m">0</span> <span class="nv">generate_node_acls</span><span class="o">=</span><span class="m">1</span> <span class="nv">cache_dynamic_acls</span><span class="o">=</span>1
</span><span class='line'>  <span class="c"># Parameter demo_mode_write_protect is now &amp;lsquo;0&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter authentication is now &amp;lsquo;0&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter generate_node_acls is now &amp;lsquo;1&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter cache_dynamic_acls is now &amp;lsquo;1&amp;rsquo;.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># sit back and marvel at your iSCSIs</span>
</span><span class='line'>  targetcli ls
</span><span class='line'>  <span class="c"># o- / &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [&amp;hellip;]</span>
</span><span class='line'>  <span class="c">#  o- backstores &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [&amp;hellip;]</span>
</span><span class='line'>  <span class="c">#  | o- block &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [Storage Objects: 1]</span>
</span><span class='line'>  <span class="c">#  | | o- block_backend  [/dev/sdb (1.0GiB) write-thru activated]</span>
</span><span class='line'>  <span class="c">#  | o- fileio &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  | o- pscsi &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  | o- ramdisk &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  o- iscsi &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Targets: 1]</span>
</span><span class='line'>  <span class="c">#  | o- iqn.2006-04.com.iscsi-is-awesome:1  [TPGs: 1]</span>
</span><span class='line'>  <span class="c">#  |   o- tpg1 &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [no-gen-acls, no-auth]</span>
</span><span class='line'>  <span class="c">#  |     o- acls &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [ACLs: 0]</span>
</span><span class='line'>  <span class="c">#  |     o- luns &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [LUNs: 1]</span>
</span><span class='line'>  <span class="c">#  |     | o- lun0  [block/block_backend (/dev/sdb)]</span>
</span><span class='line'>  <span class="c">#  |     o- portals &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [Portals: 1]</span>
</span><span class='line'>  <span class="c">#  |       o- 0.0.0.0:3260 &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [OK]</span>
</span><span class='line'>  <span class="c">#  o- loopback &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Targets: 0]</span>
</span></code></pre></td></tr></table></div></figure> </p>

<ol>
<li>iSCSI client setup
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;h1&gt;install and start the iscsi client service&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;yum install -y iscsi-initiator-utils
</span><span class='line'>systemctl start iscsid.service <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl <span class="nb">enable </span>iscsid.service&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># connect to the iscsi server</span>
</span><span class='line'>  iscsiadm -m node -o new -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># login</span>
</span><span class='line'>  iscsiadm -m node -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260 <span class="p">&amp;</span>ndash<span class="p">;</span>login
</span><span class='line'>  <span class="c"># Logging in to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] (multiple)</span>
</span><span class='line'>  <span class="c"># Login to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] successful.</span>
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Base Cluster Setup</h2>

<ol>
<li>Run the following commands on both nodes
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;h1&gt;install clustering packages&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;yum -y install pcs fence-agents-all lvm2-cluster&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># set hacluster user password</span>
</span><span class='line'>  <span class="nb">echo</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>hacluster<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">|</span> passwd hacluster <span class="p">&amp;</span>ndash<span class="p">;</span>stdin&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># start clustering services</span>
</span><span class='line'>  systemctl start pcsd.service
</span><span class='line'>  systemctl <span class="nb">enable </span>pcsd.service&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the mountpoint directory</span>
</span><span class='line'>  mkdir -p /var/opt/opscode/drbd/data
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Run the following commands on the first cluster node only
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># authorize cluster</span>
</span><span class='line'>  pcs cluster auth backend0 backend1 -u hacluster -p hacluster&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># setup cluster</span>
</span><span class='line'>  pcs cluster setup <span class="p">&amp;</span>ndash<span class="p">;</span>start <span class="p">&amp;</span>ndash<span class="p">;</span>name chef-ha backend0 backend1&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable the cluster and examine status</span>
</span><span class='line'>  pcs cluster <span class="nb">enable</span> <span class="p">&amp;</span>ndash<span class="p">;</span>all
</span><span class='line'>  pcs cluster status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable SCSI fence mode (uses SPC-3)</span>
</span><span class='line'>  pcs stonith create scsi fence_scsi <span class="nv">devices</span><span class="o">=</span>/dev/sdb meta <span class="nv">provides</span><span class="o">=</span>unfencing
</span><span class='line'>  sleep 5
</span><span class='line'>  <span class="c"># this might show stopped, try it a few times until it says started</span>
</span><span class='line'>  pcs stonith show
</span><span class='line'>  <span class="c">#  scsi (stonith:fence_scsi): Started</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. And on the 2nd cluster node:
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Option 1: Leader Election and LVM Volume failover without CLVM</h2>

<ol>
<li>on the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>  pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the LVM PV and VG</span>
</span><span class='line'>  pvcreate /dev/sdb
</span><span class='line'>  vgcreate shared_vg /dev/sdb
</span><span class='line'>  lvcreate -l 80%VG -n ha_lv shared_vg&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c">#   0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active</span>
</span><span class='line'>  lvchange -aey shared_vg
</span><span class='line'>  <span class="c">#   1 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># format the volume</span>
</span><span class='line'>  mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Update the initramfs device on all your cluster nodes, so that the CLVM volume is never auto-mounted:
</span><span class='line'> &lt;/code&gt;bash&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># Set the lvm configuration to not auto-mount anything but the root &amp;ldquo;centos&amp;rdquo; VG</span>
</span><span class='line'>  <span class="c"># yes, the 783 part means the 783rd line of the file :\</span>
</span><span class='line'>  sed -i.bak <span class="p">&amp;</span>ldquo<span class="p">;</span>783s/.*/    <span class="nv">volume_list</span> <span class="o">=</span> <span class="o">[</span> <span class="se">\&quot;</span>centos<span class="se">\&amp;</span>rdquo<span class="p">;</span>, <span class="se">\&amp;</span>ldquo<span class="p">;</span>@&lt;code&gt;hostname -s&lt;/code&gt;<span class="se">\&amp;</span>rdquo<span class="p">;</span> <span class="o">]</span>/<span class="p">&amp;</span>ldquo<span class="p">;</span> /etc/lvm/lvm.conf
</span><span class='line'>  <span class="c"># update initramfs and reboot</span>
</span><span class='line'>  dracut -H -f /boot/initramfs-<span class="k">$(</span>uname -r<span class="k">)</span>.img <span class="k">$(</span>uname -r<span class="k">)</span>
</span><span class='line'>  <span class="c"># shutdown, but use vagrant to start them up</span>
</span><span class='line'>  shutdown -h now
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. <span class="k">then</span> bring both nodes back up
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  vagrant up backend0 backend1
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>4. Add an LVM resource
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># create resources for the LVM, filesystem and IP</span>
</span><span class='line'>  pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="p">&amp;</span>rdquo<span class="p">;</span>/dev/shared_vg/ha_lv<span class="err">&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/var/opt/opscode/drbd/data<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">fstype</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>xfs<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  <span class="c">#&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># debugging - only if things are going wrong</span>
</span><span class='line'>  <span class="c"># run lvs on both nodes to ensure it only says active (&amp;ldquo;a&amp;rdquo;) on backend0</span>
</span><span class='line'>  lvs <span class="c"># on backend0</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi-ao&amp;mdash;-  3.20g</span>
</span><span class='line'>  lvs <span class="c"># on backend1</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi&amp;mdash;&amp;mdash;-  3.20g</span>
</span><span class='line'>  <span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>  bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span><span class='line'>  <span class="c"># or</span>
</span><span class='line'>  pcs resource debug-start ha_lv
</span><span class='line'>  pcs resource debug-start chef_data
</span><span class='line'>  pcs resource debug-start backend_vip
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Option 2:  Using CLVMD</h2>

<ol>
<li>On the first node: Enable clvmd and LVM clustering
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs resource create dlm ocf:pacemaker:controld clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs resource create clvmd ocf:heartbeat:clvm clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs constraint order start dlm-clone <span class="k">then</span> clvmd-clone
</span><span class='line'>pcs constraint colocation add clvmd-clone with dlm-clone&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable LVM clustering with clvmd</span>
</span><span class='line'>  lvmconf <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-cluster
</span><span class='line'>  <span class="c"># stop lvmetad</span>
</span><span class='line'>  killall lvmetad
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Run the following on the second cluster node:
</span><span class='line'> &lt;/code&gt;
</span><span class='line'>  lvmconf <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-cluster&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># stop lvmetad</span>
</span><span class='line'>  killall lvmetad
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. Back to the first cluster node:
</span><span class='line'> &lt;/code&gt;bash&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>  pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the LVM PV and VG</span>
</span><span class='line'>  pvcreate /dev/sdb
</span><span class='line'>  vgcreate -cy shared_vg /dev/sdb
</span><span class='line'>  lvcreate -l 80%VG -n ha_lv shared_vg&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c"># 0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active</span>
</span><span class='line'>  vgchange -aey shared_vg
</span><span class='line'>  <span class="c"># 1 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># run lvs on both nodes to ensure it only says active (&amp;ldquo;a&amp;rdquo;) on backend0</span>
</span><span class='line'>  lvs <span class="c"># on backend0</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi-a&amp;mdash;&amp;ndash;  3.20g</span>
</span><span class='line'>  lvs <span class="c"># on backend1</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi&amp;mdash;&amp;mdash;-  3.20g&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># format the volume and mount it</span>
</span><span class='line'>  mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>4. Add an LVM resource <span class="o">(</span>this is where shit gets whacky<span class="o">)</span>
</span><span class='line'> &lt;/code&gt;
</span><span class='line'>  <span class="c"># first unmount and deactivate</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c">#  0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/dev/shared_vg/ha_lv<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">directory</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/var/opt/opscode/drbd/data<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">fstype</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>xfs<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># debugging</span>
</span><span class='line'>  pcs resource debug-start ha_lv
</span><span class='line'>  pcs resource debug-start chef_data
</span><span class='line'>  pcs resource debug-start backend_vip
</span><span class='line'>  <span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>  bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span></code></pre></td></tr></table></div></figure> </p>

<p>Now you&rsquo;re done!</p>

<h2>Failing over</h2>

<h3>the pacemaker way</h3>

<ol>
<li>bringing down backend0</li>
<li>Option 1:  shut down backend0 <code>shutdown -h now</code></li>
<li><p>Option 2:  make it a standby via Pacemaker <code>pcs cluster standby backend0</code></p></li>
<li><p>failover of resources should be automatic to the secondary node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Cluster name: chef-ha&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Last updated: Sat Mar <span class="m">14</span> 21:55:56 2015&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Last change: Sat Mar <span class="m">14</span> 21:43:21 <span class="m">2015</span> via cibadmin on backend0&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Stack: corosync&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Current DC: backend1 <span class="o">(</span>2<span class="o">)</span> - partition with quorum&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Version: 1.1.10-32.el7_0.1-368c726&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;2 Nodes configured&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;4 Resources configured&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Online: <span class="o">[</span> backend1 <span class="o">]</span>&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;OFFLINE: <span class="o">[</span> backend0 <span class="o">]</span>&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Full list of resources:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;scsi <span class="o">(</span>stonith:fence_scsi<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Resource Group: chef_ha&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;ha_lv  <span class="o">(</span>ocf::heartbeat:LVM<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;chef_data  <span class="o">(</span>ocf::heartbeat:Filesystem<span class="o">)</span>:  Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend_vip  <span class="o">(</span>ocf::heartbeat:IPaddr2<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;PCSD Status:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend0: Offline&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend1: Online&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Daemon Status:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;corosync: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;pacemaker: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;pcsd: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="sb">```</span>&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;li&gt;Bringing backend0 back up and logging in, it won<span class="p">&amp;</span>rsquo<span class="p">;</span>t want to run pacemaker
</span></code></pre></td></tr></table></div></figure> bash
pcs status

<h1>Error: cluster is not currently running on this node</h1>

<p>systemctl start pacemaker</p></li>
</ol>


<p>  # now checking status
  pcs status
  # Cluster name: chef-ha
  # Last updated: Sat Mar 14 22:06:25 2015
  # Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0
  # Stack: corosync
  # Current DC: backend1 (2) - partition with quorum
  # Version: 1.1.10-32.el7_0.1-368c726
  # 2 Nodes configured
  # 4 Resources configured
  #
  #
  # Node backend0 (1): pending
  # Online: [ backend1 ]
  #
  # Full list of resources:
  #
  #  scsi (stonith:fence_scsi): Started backend1
  #  Resource Group: chef_ha
  #      ha_lv  (ocf::heartbeat:LVM): Started backend1
  #      chef_data  (ocf::heartbeat:Filesystem):  Started backend1
  #      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1
  #
  # PCSD Status:
  #   backend0: Online
  #   backend1: Online
  #
  # Daemon Status:
  #   corosync: active/enabled
  #   pacemaker: active/enabled
  #   pcsd: active/enabled
  <code>
4. If backend0 is considered standby, you can just unstandby it:
 </code>bash
  pcs cluster unstandby backend0</p>

<p>  pcs status
  # Cluster name: chef-ha
  # Last updated: Sat Mar 14 22:11:53 2015
  # Last change: Sat Mar 14 22:11:52 2015 via crm_attribute on backend0
  # Stack: corosync
  # Current DC: backend1 (2) - partition with quorum
  # Version: 1.1.10-32.el7_0.1-368c726
  # 2 Nodes configured
  # 4 Resources configured
  #
  #
  # Online: [ backend0 backend1 ]
  <code>
5. What happens if you try to mount the disk on the inactive node?
 </code>bash
  # LVM will be nice, but won&rsquo;t let you
  vgchange -aey shared_vg
  # 0 logical volume(s) in volume group &ldquo;shared_vg&rdquo; now active</p>

<p>  lvchange -aey shared_vg/ha_lv
  lvs
  # LV    VG        Attr       LSize   Pool Origin Data%  Move Log Cpy%Sync Convert
  # root  centos    -wi-ao&mdash;-  38.48g
  # swap  centos    -wi-ao&mdash;-   1.03g
  # ha_lv shared_vg -wi&mdash;&mdash;- 816.00m
  <figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'><span class="nt">&lt;h3&gt;</span>the CLVM way<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;ol&gt;</span>
</span><span class='line'><span class="nt">&lt;li&gt;</span>On the active node
</span></code></pre></td></tr></table></div></figure> 
[root@backend0 ~]# umount /var/opt/opscode/drbd/data
[root@backend0 ~]# lvchange -an shared_vg/ha_lv</li>
</ol>


<p>  # ensure the LV isn&rsquo;t active
  [root@backend0 ~]# lvs
    LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
    root  centos    -wi-ao&mdash;- 38.48g
    swap  centos    -wi-ao&mdash;-  1.03g
    ha_lv shared_vg -wi&mdash;&mdash;-  3.20g
  <code>
2. on the standby node
 </code>bash</p>

<p>  [root@backend1 ~]# lvs
    LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
    root  centos    -wi-ao&mdash;- 38.48g
    swap  centos    -wi-ao&mdash;-  1.03g
    ha_lv shared_vg -wi&mdash;&mdash;-  3.20g
  [root@backend1 ~]# lvchange -aey shared_vg/ha_lv
  [root@backend1 ~]# mount /dev/shared_vg/ha_lv /var/opt/opscode/drbd/data
  <figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'><span class="nt">&lt;h3&gt;</span>Forcing yourself to go standby<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>TBD<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h3&gt;</span>Forcing the other node to go standby (STONITH)<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>TBD<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h2&gt;</span>Troubleshooting<span class="nt">&lt;/h2&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h3&gt;</span>Do SCSI SPC-3 persistent reservations work on my device?<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>good:
</span></code></pre></td></tr></table></div></figure> bash</p>

<h1>exampling talking to the Linux iSCSI service:</h1>

<p>sg_persist &ndash;in &ndash;report-capabilities -v /dev/sdb</p>

<h1>inquiry cdb: 12 00 00 00 24 00</h1>

<h1>LIO-ORG   block_backend     4.0</h1>

<h1>Peripheral device type: disk</h1>

<h1>Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</h1>

<h1>Report capabilities response:</h1>

<h1>Compatible Reservation Handling(CRH): 1</h1>

<h1>Specify Initiator Ports Capable(SIP_C): 1</h1>

<h1>All Target Ports Capable(ATP_C): 1</h1>

<h1>Persist Through Power Loss Capable(PTPL_C): 1</h1>

<h1>Type Mask Valid(TMV): 1</h1>

<h1>Allow Commands: 1</h1>

<h1>Persist Through Power Loss Active(PTPL_A): 0</h1>

<h1>Support indicated in Type mask:</h1>

<h1>Write Exclusive, all registrants: 1</h1>

<h1>Exclusive Access, registrants only: 1</h1>

<h1>Write Exclusive, registrants only: 1</h1>

<h1>Exclusive Access: 1</h1>

<h1>Write Exclusive: 1</h1>

<h1>Exclusive Access, all registrants: 1</h1>

<pre><code>
bad:
</code></pre>

<h1>example using Virtualbox SCSI/SAS/etc disks which are not SPC-compliant:</h1>

<p>sg_persist &ndash;in &ndash;report-capabilities -v /dev/sdb</p>

<h1>inquiry cdb: 12 00 00 00 24 00</h1>

<h1>VBOX      HARDDISK          1.0</h1>

<h1>Peripheral device type: disk</h1>

<h1>Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</h1>

<h1>persistent reservation in:  Fixed format, current;  Sense key: Illegal Request</h1>

<h1>Additional sense: Invalid command operation code</h1>

<h1>Info fld=0x0 [0]</h1>

<h1>PR in (Report capabilities): command not supported</h1>

<p>```</p>

<p>TBD</p>
]]></content>
  </entry>
  
</feed>
