<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Chef | Irving's blog]]></title>
  <link href="http://irvingpop.github.io/blog/categories/chef/atom.xml" rel="self"/>
  <link href="http://irvingpop.github.io/"/>
  <updated>2015-04-22T13:19:22-07:00</updated>
  <id>http://irvingpop.github.io/</id>
  <author>
    <name><![CDATA[Irving Popovetsky]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Tuning the Chef Server for Scale]]></title>
    <link href="http://irvingpop.github.io/blog/2015/04/20/tuning-the-chef-server-for-scale/"/>
    <updated>2015-04-20T13:51:36-07:00</updated>
    <id>http://irvingpop.github.io/blog/2015/04/20/tuning-the-chef-server-for-scale</id>
    <content type="html"><![CDATA[<p>In Chef&rsquo;s Customer Engineering team we are frequently asked for advice on tuning the Chef Server for high-scale situations. Below is a summary of what we generally tell customers. Note that these tuning settings are specific to Chef Server 12, which is the recommended version for any customer who cares about the performance of their Chef server.</p>

<!-- more -->


<h2>General Advice</h2>

<h3>Understand the OSS components that make up the Chef Server</h3>

<p>A good way to think about the Chef server is as a collection of Microservices components underpinned by OSS software:</p>

<ul>
<li>Nginx (openresty)</li>
<li>PostgreSQL</li>
<li>Solr</li>
<li>RabbitMQ</li>
<li>Redis</li>
<li>Chef</li>
<li>Erlang/OTP</li>
<li>Ruby</li>
<li>The Linux Kernel

<ul>
<li>LVM</li>
<li>Storage subsystem</li>
<li>Network stack</li>
</ul>
</li>
</ul>


<p>It&rsquo;s important to understand the performance characteristics, monitoring and troubleshooting of these components.  Especially Postgres, Solr, RabbitMQ and Linux systems in general. It&rsquo;s worth noting that the Chef server core is Open Source, and all if its code can be examined <a href="https://github.com/chef/chef-server">on Github</a></p>

<p>Because these components are glued together using Chef, it&rsquo;s highly recommended that you familiarize yourself with the <a href="https://github.com/chef/opscode-omnibus/tree/master/files/private-chef-cookbooks">cookbooks that configure the Chef server when you run <code>chef-server-ctl reconfigure</code></a></p>

<h3>Have good monitoring in place</h3>

<p>We don&rsquo;t provide prescriptive monitoring guidance at this time, but here&rsquo;s our advice:</p>

<ul>
<li>Use existing Open source software (Sensu, Nagios, etc) to collect metrics and test the health of the OSS components.  This should be fairly straightforward to set up.

<ul>
<li>Use <a href="http://dalibo.github.io/pgbadger/">pgBadger</a> for Postgres log analysis</li>
<li>Install the <a href="https://www.rabbitmq.com/management.html">RabbitMQ Management Plugin</a> for detailed monitoring of RabbitMQ</li>
</ul>
</li>
<li>Configure your monitoring systems and load balancers to query the Health status endpoint of erchef (<a href="https://mychefserver/_status">https://mychefserver/_status</a>)</li>
<li>Run a graphite server. erchef will send detailed statistics if you set the following in your <code>chef-server.rb</code> file:
<code>ruby
folsom_graphite['enable'] = true
folsom_graphite['host'] = 'graphite.mycompany.com'
folsom_graphite['port'] = 2003
</code></li>
<li>Use Splunk or Logstash to collect and analyze your Chef server logs.

<ul>
<li>You can collect and graph useful performance data by graphing <code>/var/log/opscode/opscode-erchef/requests.log.N</code>, <code>/var/log/opscode/oc_bifrost/requests.log.N</code> and <code>/var/log/opscode/opscode-reporting/requests.log.N</code></li>
<li>Each request line will show (in ms) various performance counter. For example: <code>req_time=20; rdbms_time=2; rdbms_count=3; authz_time=5; authz_count=1;</code></li>
</ul>
</li>
</ul>


<h3>Think about API requests per second rather than node counts</h3>

<p>A very common measurement for the size of Chef servers/clusters is the number of nodes they serve. However, this number is not terribly useful because of other elements that can cause very wide variation. Namely:</p>

<ul>
<li>The interval and splay of Chef client runs

<ul>
<li>1000 nodes every hour == 500 nodes every 30 minutes</li>
<li>Insufficient splay can cause a &ldquo;stampede condition&rdquo; on the Chef server. Splay should be equal to the interval in order to get maximum smoothness of request load.</li>
</ul>
</li>
<li>The number and complexity of search requests and databag fetches performed during each Chef run</li>
<li>The number of cookbooks depended on for each Chef run. More cookbooks adds loading to the depsolver and also to the Bookshelf service which serves cookbooks</li>
<li>The size of node data, which we&rsquo;ve seen range from 32kb to 5MB (the default maximum is 1MB but can be increased). This adds load to the indexing service (opscode-expander) as well as to Solr</li>
</ul>


<p>Although it&rsquo;s not perfect, we&rsquo;ve found that a good rule of thumb for examining active Chef servers is the number of API requests per second aggregated across the entire cluster. We&rsquo;ve found that clusters which sustained higher than 125 API RPS started to experience occasional errors.</p>

<h3>DRBD: Don&rsquo;t do it</h3>

<p>In the field we&rsquo;ve found that DRBD has a negative impact on performance and availability of Chef server clusters. Specifically:</p>

<ul>
<li>Because DRBD uses synchronous replication, a block is not considered &ldquo;comitted to disk&rdquo; until it was been confirmed by both nodes in the cluster. This adds significant latency to each IOP.</li>
<li>DRBD&rsquo;s bandwidth is limited by the network throughput between the nodes. Dedicated cross-over links are not possible in all scenarios (for example VMs) which leads to low and inconsistent throughput.</li>
<li>DRBD resyncs can take a very long time and greatly impact performance while running.</li>
<li>Although DRBD protects against hardware failure, it does a very poor job of protecting against many classes of software failure. For example, a corrupt database is replicated whole to the other node, so failing over will not correct the system.</li>
</ul>


<h3>Beware LVM snapshots impact on performance</h3>

<p>LVM is generally recommended for storing all Chef Server data (<code>/var/opt/opscode</code> in standalone/tier installs and <code>/var/opt/opscode/drbd/data</code> in HA installs) because it provides the ability to expand disks on the fly and create crash-consistent snapshots.</p>

<p>However it&rsquo;s important to know that as LVM snapshots increase in size it is very detrimental to performance:</p>

<ul>
<li><a href="http://www.percona.com/blog/2013/07/09/lvm-read-performance-during-snapshots/">http://www.percona.com/blog/2013/07/09/lvm-read-performance-during-snapshots/</a></li>
<li><a href="http://www.percona.com/blog/2009/02/05/disaster-lvm-performance-in-snapshot-mode/">http://www.percona.com/blog/2009/02/05/disaster-lvm-performance-in-snapshot-mode/</a></li>
</ul>


<p>Therefore it is recommend that snapshots are used to create consistent backups, but are immediately deleted after they are no longer needed.</p>

<h2>Chef Server tuning tips</h2>

<h3>Server sizing</h3>

<p><strong>Chef Server frontends:</strong></p>

<ul>
<li>Frontends run stateless services only (erchef, bifrost, reporting, manage) and can be scaled horizontally.</li>
<li>They are almost always CPU bound, and only suffer memory or disk pressure during fault scenarios (typically because of backend issues).</li>
<li>A good starting point for frontends is 4 CPU cores and 8 GB RAM. Disk on frontends does not matter.</li>
</ul>


<p><strong>Chef server backends:</strong></p>

<ul>
<li>Backends mix a number of disk, memory and CPU bound services (Postgres, Solr, RabbitMQ, Expander)</li>
<li>A good starting point for backends is 8 CPU cores and 32 GB of RAM.</li>
<li>Flash-based storage is highly recommend, combined with the XFS filesystem and LVM.</li>
</ul>


<h3>chef-server.rb tuning settings</h3>

<p><strong>Database pooling:</strong></p>

<p>In the erlang OTP process model, the number of workers is limited by the size of the database connection pool (default 20). Increasing the database pool allows for more workers, but puts added memory pressure on the database service.</p>

<p>In order to handle the greater number of connections, you must also increase the Postgres <code>max_connections</code> value. This value must consider an erchef, bifrost and reporting process connecting from each frontend, plus an extra 20% for breathing room.</p>

<p>Suggested values for a high-performing cluster with 4-6 frontends:
<code>ruby
postgresql['max_connections'] = 1024
opscode_erchef['db_pool_size'] = 60
oc_bifrost['db_pool_size'] = 60
</code></p>

<p><strong>Erchef to bifrost http connection pool:</strong>
erchef also maintains a pool of http connections to bifrost, the authz service.  It&rsquo;s important to raise the initial and maximum number of connections with respect to the database pool sizes.</p>

<pre><code class="ruby">oc_chef_authz['http_init_count'] = 100
oc_chef_authz['http_max_count'] = 200
</code></pre>

<p><strong>Erchef depsolver and keygen tuning:</strong>
Two expensive computations that erchef must perform are the depsolver (a Ruby process which solves the cookbook dependencies) as well as the client key generator (which can be hit hard when large fleets of chef nodes are provisioned)</p>

<p>Suggested values:
<code>ruby
opscode_erchef['depsolver_worker_count'] = 4 # should equal the number of CPU cores
opscode_erchef['depsolver_timeout'] = 120000
opscode_erchef['keygen_cache_size'] = 1000
</code></p>

<p><strong>Nginx cookbook caching:</strong>
A new feature in Chef Server 12.0.4 is <a href="https://www.chef.io/blog/2015/02/18/cookbook-caching/">Nginx cookbook caching</a>. This takes load off of the backend Bookshelf service by storing cookbook files in Nginx.</p>

<p>Suggested values:
<code>ruby
opscode_erchef['nginx_bookshelf_caching'] = ":on"
opscode_erchef['s3_url_expiry_window_size'] = "100%"
</code></p>

<p><strong>PostgreSQL tuning:</strong>
We already tune PostgreSQL memory settings to sane values based on the backend&rsquo;s phyiscal RAM. For example, <code>effective_cache_size</code> is set to 50% of RAM, and <code>shared_buffers</code> to 25% of physical RAM.</p>

<p>To handle the heavy write load on large clusters, it is recommended to tune the checkpointer.</p>

<p>Suggested values:
<code>ruby
postgresql['checkpoint_segments'] = 100
postgresql['checkpoint_completion_target'] = 0.9
</code></p>

<p><strong>Solr JVM tuning:</strong>
By default we compute Solr&rsquo;s JVM heap size to be either 25% of system memory or 1024MB, whichever is smaller. Large chef server clusters should increase this value to smaller of 25% of system memory or 4096MB.</p>

<p>Suggested values:
<code>ruby
opscode_solr4['heap_size'] = 4096
opscode_solr4['new_size'] = 256
</code></p>

<p><em>WARNING: It is not recommended to use a JVM heap_size above 8GB, at that level the cost of Garbage Collection becomes too high and impacts performance worse than using a smaller heap</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Setting Up Your Private Supermarket Server]]></title>
    <link href="http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server/"/>
    <updated>2015-04-07T15:48:34-07:00</updated>
    <id>http://irvingpop.github.io/blog/2015/04/07/setting-up-your-private-supermarket-server</id>
    <content type="html"><![CDATA[<p><em>This is an updated version of the previous post from August, 2014: <a href="https://www.chef.io/blog/2014/08/29/getting-started-with-oc-id-and-supermarket/">Getting started with oc-id and Supermarket</a></em></p>

<p>Chef Server 12 includes <a href="https://github.com/chef/oc-id">oc-id</a>, the OAuth2 service that powers <a href="https://id.chef.io/">id.chef.io</a>.  After upgrading to this release, Chef customers can now run their own Supermarket service behind a firewall.</p>

<!-- more -->


<h2>oc-id setup on your Chef Server:</h2>

<p><em>You must be logged in to your Chef server via ssh and elevated to an admin user level for the following steps</em></p>

<ol>
<li><p>Add the following setting to your <code>/etc/opscode/chef-server.rb</code> configuration file:
<code>ruby
oc_id['applications'] = {
'supermarket' =&gt; {
  'redirect_uri' =&gt; 'https://supermarket.mycompany.com/auth/chef_oauth2/callback'
}
}
</code></p></li>
<li><p>run <code>chef-server-ctl reconfigure</code></p></li>
<li><p>After the reconfigure, you will find the OAuth2 data in <code>/etc/opscode/oc-id-applications/supermarket.json</code>
<code>json
{
 "name": "supermarket",
 "uid": "0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b",
 "secret": "17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879",
 "redirect_uri": "https://supermarket.mycompany.com/auth/chef_oauth2/callback"
}
</code></p></li>
</ol>


<p>Note the <code>uid</code> and <code>secret</code> values from this file, you will need them for the next stage.</p>

<p><em>You can add as many oc-id applications as you wish to the chef-server.rb configuration, it will create one file per application</em></p>

<h2>Running your Private Supermarket server in Test Kitchen</h2>

<p><em>Note: We will not use the community Supermarket cookbook, because at this time it installs Supermarket from source.  Instead, we will us an Omnibus package to install</em></p>

<p>In the spirit of &ldquo;code as documentation&rdquo; I&rsquo;ve provided a simple cookbook and test-kitchen configuration for testing Supermarket Omnibus packages. These packages are downloaded from <a href="https://packagecloud.io/chef/stable">https://packagecloud.io/chef/stable</a></p>

<ol>
<li><p>Download a copy of the [supermarket-omnibus-cookbook]<a href="https://github.com/irvingpop/supermarket-omnibus-cookbook">https://github.com/irvingpop/supermarket-omnibus-cookbook</a>
<code>bash
git clone https://github.com/irvingpop/supermarket-omnibus-cookbook.git supermarket-omnibus-cookbook
cd supermarket-omnibus-cookbook
</code></p></li>
<li><p>Create a <code>.kitchen.local.yml</code> file, to set your oc-id attributes (as captured in step 3 above)
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">&lt;/ol&gt;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">&lt;hr /&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">&lt;p&gt;  suites</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">default</span>
</span><span class='line'>      <span class="l-Scalar-Plain">run_list</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="p-Indicator">-</span> <span class="l-Scalar-Plain">recipe[supermarket-omnibus-cookbook::default]</span>
</span><span class='line'>      <span class="l-Scalar-Plain">attributes</span><span class="p-Indicator">:</span>
</span><span class='line'>        <span class="l-Scalar-Plain">supermarket_omnibus</span><span class="p-Indicator">:</span>
</span><span class='line'>          <span class="l-Scalar-Plain">chef_server_url</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">&lt;a href=&quot;https://chefserver.mycompany.com&quot;&gt;https://chefserver.mycompany.com&lt;/a&gt;</span>
</span><span class='line'>          <span class="l-Scalar-Plain">chef_oauth2_app_id</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">0bad0f2eb04e935718e081fb71e3b7bb47dc3681c81acb9968a8e1e32451d08b</span>
</span><span class='line'>          <span class="l-Scalar-Plain">chef_oauth2_secret</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">17cf1141cc971a10ce307611beda7f4dc6633bb54f1bc98d9f9ca76b9b127879</span>
</span><span class='line'>          <span class="l-Scalar-Plain">chef_oauth2_verify_ssl</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">false</span>
</span></code></pre></td></tr></table></div></figure> </p>

<ol>
<li><p>Install the <code>vagrant-hostsupdater</code> plugin, this will automatically add the names of your machines to your /etc/hosts file. This is important for oauth2, which cares about host names. The <code>redirect_uri</code> value you entered in to your oc-id configuration reflects this name.
<code>bash
vagrant plugin install vagrant-hostsupdater
</code></p></li>
<li><p>Start your Supermarket instance and test it
<code>bash
kitchen converge default-centos-66 &amp;&amp; kitchen verify default-centos-66
</code></p></li>
<li><p>Go to your your Supermarket server and log in as a Chef user: <a href="https://default-centos-66">https://default-centos-66</a></p></li>
<li><p>Upon login, you should see:
<img class="<a" src="href="https://www.getchef.com/blog/wp-content/uploads/2014/08/oc-id5-1024x343.png">https://www.getchef.com/blog/wp-content/uploads/2014/08/oc-id5-1024x343.png</a>"></p></li>
</ol>


<h2>Uploading your first cookbook to Supermarket</h2>

<ol>
<li>Install the <a href="https://github.com/chef/knife-supermarket">knife-supermarket</a> gem. In ChefDK:
<code>bash
chef gem install knife-supermarket
</code></li>
<li>In your <code>knife.rb</code> file, add a setting for the supermarket server:
<code>ruby
knife[:supermarket_site] = 'https://default-centos-66'
</code></li>
<li>To resolve any SSL errors, fetch and verify the Supermarket server&rsquo;s SSL certificate:
<code>bash
knife ssl fetch https://default-centos-66
knife ssl check https://default-centos-66
</code></li>
<li>Upload your cookbook to Supermarket
<code>bash
knife supermarket share mycookbook "Other"
</code></li>
</ol>


<h2>Running Supermarket in Production</h2>

<p>Supermarket is still in early stages and does not have official Support from Chef, HA, backup tools, etc.  Although several of our key customers are running Supermarket in prod, they are doing it at their own risk.</p>

<p>In general we recommend that you start using small VMs, it&rsquo;s easy to increase your VM size as you need it. Put your <code>/var/opt/supermarket</code> directory on a separate disk and use LVM so that it can be expanded.</p>

<h3>Your Wrapper Cookbook attributes</h3>

<p>We recommend that you use use a wrapper cookbook with role recipes to deploy Supermarket.</p>

<p>All of the keys under <code>node['supermarket_omnibus']</code> are written out as <code>/etc/supermarket/supermarket.json</code>.  You can add others as you see fit to override the defaults specified in the <a href="https://github.com/chef/omnibus-supermarket/blob/master/cookbooks/omnibus-supermarket/attributes/default.rb">supermarket Omnibus package</a>
<code>ruby
default['supermarket_omnibus']['chef_server_url'] = 'https://chefserver.mycompany.com'
default['supermarket_omnibus']['chef_oauth2_app_id'] = '14dfcf186221781cff51eedd5ac1616'
default['supermarket_omnibus']['chef_oauth2_secret'] = 'a49402219627cfa6318d58b13e90aca'
default['supermarket_omnibus']['chef_oauth2_verify_ssl'] = false
</code></p>

<h3>Scaling the system</h3>

<p>Supermarket is a Ruby on Rails app with a Postgres backend, and typical RoR scaling rules apply.  If you wish to run Supermarket in a scale-out or HA mode, you can do this by building our your own back-end components:</p>

<ul>
<li><strong>Database:</strong> Build a separate PostgreSQL 9.3+ server (or HA pair). Please note that the following Postgres extensions must be installed and loaded: <code>pgpsql</code> and <code>pg_trgm</code></li>
<li><strong>Cookbook Storage</strong> Cookbook tarballs are stored by default in <code>/var/opt/supermarket/data</code>. You can change this to use Amazon S3 (recommended) or an <a href="http://stackoverflow.com/questions/10574909/is-there-an-open-source-equivalent-to-amazon-s3">S3-compatible service</a>. If those are not an option you can symlink this directory to shared storage (e.g. NFS) although this has not been fully tested against race conditions.</li>
<li><strong>(Optional) Caching Service:</strong> Supermarket uses Redis as its caching service. You can safely run one Redis instance per Supermarket app server, or you can choose to run a Redis 2.8+ server or HA pair.</li>
</ul>


<h2>Troubleshooting &amp; FAQ</h2>

<h3>Incorrect redirect URL</h3>

<p>The redirect URL specified in oc-id <strong>MUST</strong> match the hostname of the Supermarket server. Also, you must get the URI correct (/auth/chef_oauth2/callback). If these are not true, you will recieve an error message like:
<code>
The redirect uri included is not valid.
</code></p>

<h3>Supermarket server cannot reach oc-id, throws 500 error during login</h3>

<p>The Supermarket server must be able to reach (via https) the specified <code>chef_server_url</code> - it does this during OAuth2 negotation. The most common problems are name resolution and firewall rules.</p>

<h3>Where can I find the code to Supermarket?</h3>

<ul>
<li>Supermarket the rails application is <a href="https://github.com/chef/supermarket">located here</a>

<ul>
<li>All Supermarket <a href="https://github.com/chef/supermarket/issues">issues should be reported there</a></li>
</ul>
</li>
<li>The code which builds Supermarket into an Omnibus package is <a href="https://github.com/chef/omnibus-supermarket">located here</a>

<ul>
<li>The cookbook that is run when during <code>supermarket-ctl reconfigure</code> is <a href="https://github.com/chef/omnibus-supermarket/tree/master/cookbooks/omnibus-supermarket">located within this repo</a></li>
<li>You can build your own Omnibus packages by following <a href="https://github.com/chef/omnibus-supermarket#kitchen-based-build-environment">the instructions in the README.md</a></li>
</ul>
</li>
</ul>


<h3>How do I enable rails application debug logging?</h3>

<p>There is a known issue with the Supermarket omnibus package that rails messages are not logged. To fix that requires a manual change at the moment. On your supermarket server, edit this file: <code>/opt/supermarket/embedded/service/supermarket/config/environments/production.rb</code>, change line 46 (<code>config.log_level = :warn</code>) to look like:
<code>ruby
  config.logger = Logger.new('/var/log/supermarket/rails/rails.log')
  config.logger.level = 'DEBUG'
  config.log_level = :debug
</code></p>

<p>Then restart the rails service by running
<code>supermarket-ctl restart rails</code></p>

<h3>Contacting packagecloud fails if I&rsquo;m behind a proxy</h3>

<p>No problem!  Add the following to your <code>.kitchenl.local.yml</code> file:</p>

<pre><code class="yaml">---
provisioner:
  name: chef_zero
  solo_rb:
    http_proxy: http://192.168.1.1
    https_proxy: http://192.168.2.2
</code></pre>

<h3>Test kitchen is slow because it has to download/install the Chef Omnibus client package every time</h3>

<p>Here&rsquo;s a few tips to speed it up:</p>

<ol>
<li>Tell test-kitchen to cache the Omnibus installer (put this in your <code>.kitchen.local.yml</code> file):
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
</pre></td><td class='code'><pre><code class='yaml'><span class='line'><span class="l-Scalar-Plain">&lt;/ol&gt;</span>
</span><span class='line'>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">&lt;hr /&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="l-Scalar-Plain">&lt;p&gt;  provisioner</span><span class="p-Indicator">:</span>
</span><span class='line'>    <span class="l-Scalar-Plain">name</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">chef_zero</span>
</span><span class='line'>    <span class="l-Scalar-Plain">chef_omnibus_install_options</span><span class="p-Indicator">:</span> <span class="l-Scalar-Plain">-d /tmp/vagrant-cache/vagrant_omnibus</span>
</span><span class='line'><span class="-Error">  </span><span class="l-Scalar-Plain">&lt;code&gt;</span>
</span><span class='line'><span class="l-Scalar-Plain">2. Cache yum repos like packagecloud using the vagrant-cachier plugin.  First run `vagrant plugin install vagrant-cachier`, then create a `$VAGRANT_HOME/Vagrantfile` that looks like so</span><span class="p-Indicator">:</span>
</span><span class='line'> <span class="l-Scalar-Plain">&lt;/code&gt;ruby</span>
</span><span class='line'>  <span class="l-Scalar-Plain">Vagrant.configure(&amp;ldquo;2&amp;rdquo;) do |config|</span>
</span><span class='line'>    <span class="l-Scalar-Plain">config.vm.box = &amp;lsquo;some-box&amp;rsquo;</span>
</span><span class='line'>    <span class="l-Scalar-Plain">if Vagrant.has_plugin?(&amp;ldquo;vagrant-cachier&amp;rdquo;)</span>
</span><span class='line'>      <span class="l-Scalar-Plain">config.cache.scope = :box</span>
</span><span class='line'>      <span class="l-Scalar-Plain">config.cache.enable :chef</span>
</span><span class='line'>      <span class="l-Scalar-Plain">config.cache.enable :apt</span>
</span><span class='line'>      <span class="l-Scalar-Plain">config.cache.enable :yum</span>
</span><span class='line'>      <span class="l-Scalar-Plain">config.cache.enable :gem</span>
</span><span class='line'>    <span class="l-Scalar-Plain">end</span>
</span><span class='line'>  <span class="l-Scalar-Plain">end</span>
</span></code></pre></td></tr></table></div></figure> </p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Experimenting With Redhat Cluster Suite 7]]></title>
    <link href="http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments/"/>
    <updated>2015-03-01T14:48:34-08:00</updated>
    <id>http://irvingpop.github.io/blog/2015/03/01/redhat7-clustering-experiments</id>
    <content type="html"><![CDATA[<h2>Description</h2>

<p>Source code at: <a href="https://github.com/irvingpop/rhcs7">https://github.com/irvingpop/rhcs7</a></p>

<p>I created a Vagrant-based configuration that brings up two CentOS 7 nodes that share a cluster disk with CLVM.
Based on:  <a href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html">https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/High_Availability_Add-On_Administration/ch-startup-HAAA.html</a>
with clarification from: <a href="http://www.davidvossel.com/wiki/index.php?title=HA_LVM">http://www.davidvossel.com/wiki/index.php?title=HA_LVM</a></p>

<p>Ultimate goal: Find a way to couple Redhat clustering with Chef Server to create a more robust HA stack.</p>

<!-- more -->


<h2>Notes</h2>

<ul>
<li>Redhat clustering is HARD and not for the faint of heart.  It&rsquo;s not nearly as well documented as it should be, there&rsquo;s quite a few closely interconnected services and you should really take time out to understand it on its own before combining it with anything else.

<ul>
<li>Redhat clustering in EL5 and EL6 was pretty awful to operate and hard to understand.  It has been majorly redone in RHEL7 and is now considerably simpler to get going.</li>
</ul>
</li>
<li>This document and repo focus on the <em>shared storage</em> use case, where two nodes have access to the same block device but only one node can actively mount and write to it at a time. The simultaneous access use cases (GFS, OCFS, etc) are not covered here.</li>
<li>Initially I tried to use Virtualbox &ldquo;shareable&rdquo; block devices but this didn&rsquo;t work because it doesn&rsquo;t support the SCSI SPC-3 feature set.  This is important because the only applicable fence agent I could use is fence_scsi, which uses SCSI SPC-3 persistent reservations ( <a href="https://kb.netapp.com/support/index?page=content&amp;id=3012956">https://kb.netapp.com/support/index?page=content&amp;id=3012956</a> )</li>
<li>I switched to using the Linux iSCSI service on a separate node to get SPC-3.  In production you shouldn&rsquo;t use iSCSI.  It will burn your beans, melt your ice cream, and in general disappoint you continuously.  Yes, Fibre Channel is expensive and iSCSI is cheap, but over a decade of storage management experience has taught me that there&rsquo;s a good reason for that.</li>
</ul>


<p>There&rsquo;s two ways to do HA-LVM:
* Exclusive Activation via LVM volume_list filtering: This basically tells LVM not to auto-activate the shared storage, and then uses a tagging system (plus trust in pacemaker) to control exclusive access
* Exclusive Activation via clvmd: This uses the clvmd service (plus a distributed lock manager) to control access to LVM volume groups which are specially tagged as &ldquo;clustered&rdquo;
* Irving&rsquo;s thoughts: clvmd would appear to be the safer way to go because it protects against &ldquo;rogue&rdquo; nodes, but in practice clvmd is really freakin hard to figure out and troubleshoot. Also, the SCSI SPC-3 persistent reservations provide an additional layer of safety.  So I recommend Option 1, the volume_list filtering approach as it gave me far less heartburn in testing.</p>

<h2>Initial setup</h2>

<ol>
<li>Initial configuration of the cluster machines, backend0 and backend1:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  vagrant up
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>1. iSCSI server setup
</span><span class='line'>from: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Storage_Administration_Guide/ch25.html#osm-target-setup
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># install</span>
</span><span class='line'>  yum install -y targetcli
</span><span class='line'>  <span class="c"># start and enable the service</span>
</span><span class='line'>  systemctl start target <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl <span class="nb">enable </span>target&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># map the backstore to a physical unformatted block device</span>
</span><span class='line'>  targetcli /backstores/block create <span class="nv">name</span><span class="o">=</span>block_backend <span class="nv">dev</span><span class="o">=</span>/dev/sdb&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create an iSCSI target</span>
</span><span class='line'>  targetcli /iscsi create iqn.2006-04.com.iscsi-is-awesome:1
</span><span class='line'>  <span class="c">#  Created target iqn.2006-04.com.iscsi-is-awesome:1</span>
</span><span class='line'>  <span class="c">#  Created TPG 1.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the iSCSI portal (IP listener)</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/portals/ create
</span><span class='line'>  <span class="c"># Using default IP port 3260</span>
</span><span class='line'>  <span class="c"># Binding to INADDR_ANY (0.0.0.0)</span>
</span><span class='line'>  <span class="c"># Created network portal 0.0.0.0:3260.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># Map the iSCSI target to the backstore</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/luns/ create /backstores/block/block_backend
</span><span class='line'>  <span class="c"># Created LUN 0.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># crazy wild-west no-ACL mode, because this is a PoC :)</span>
</span><span class='line'>  targetcli /iscsi/iqn.2006-04.com.iscsi-is-awesome:1/tpg1/ <span class="nb">set </span>attribute <span class="nv">authentication</span><span class="o">=</span><span class="m">0</span> <span class="nv">demo_mode_write_protect</span><span class="o">=</span><span class="m">0</span> <span class="nv">generate_node_acls</span><span class="o">=</span><span class="m">1</span> <span class="nv">cache_dynamic_acls</span><span class="o">=</span>1
</span><span class='line'>  <span class="c"># Parameter demo_mode_write_protect is now &amp;lsquo;0&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter authentication is now &amp;lsquo;0&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter generate_node_acls is now &amp;lsquo;1&amp;rsquo;.</span>
</span><span class='line'>  <span class="c"># Parameter cache_dynamic_acls is now &amp;lsquo;1&amp;rsquo;.&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># sit back and marvel at your iSCSIs</span>
</span><span class='line'>  targetcli ls
</span><span class='line'>  <span class="c"># o- / &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [&amp;hellip;]</span>
</span><span class='line'>  <span class="c">#  o- backstores &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [&amp;hellip;]</span>
</span><span class='line'>  <span class="c">#  | o- block &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [Storage Objects: 1]</span>
</span><span class='line'>  <span class="c">#  | | o- block_backend  [/dev/sdb (1.0GiB) write-thru activated]</span>
</span><span class='line'>  <span class="c">#  | o- fileio &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  | o- pscsi &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  | o- ramdisk &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Storage Objects: 0]</span>
</span><span class='line'>  <span class="c">#  o- iscsi &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Targets: 1]</span>
</span><span class='line'>  <span class="c">#  | o- iqn.2006-04.com.iscsi-is-awesome:1  [TPGs: 1]</span>
</span><span class='line'>  <span class="c">#  |   o- tpg1 &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [no-gen-acls, no-auth]</span>
</span><span class='line'>  <span class="c">#  |     o- acls &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [ACLs: 0]</span>
</span><span class='line'>  <span class="c">#  |     o- luns &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [LUNs: 1]</span>
</span><span class='line'>  <span class="c">#  |     | o- lun0  [block/block_backend (/dev/sdb)]</span>
</span><span class='line'>  <span class="c">#  |     o- portals &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;. [Portals: 1]</span>
</span><span class='line'>  <span class="c">#  |       o- 0.0.0.0:3260 &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;.. [OK]</span>
</span><span class='line'>  <span class="c">#  o- loopback &amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; [Targets: 0]</span>
</span></code></pre></td></tr></table></div></figure> </p>

<ol>
<li>iSCSI client setup
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;h1&gt;install and start the iscsi client service&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;yum install -y iscsi-initiator-utils
</span><span class='line'>systemctl start iscsid.service <span class="p">&amp;</span>amp<span class="p">;&amp;</span>amp<span class="p">;</span> systemctl <span class="nb">enable </span>iscsid.service&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># connect to the iscsi server</span>
</span><span class='line'>  iscsiadm -m node -o new -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># login</span>
</span><span class='line'>  iscsiadm -m node -T iqn.2006-04.com.iscsi-is-awesome:1 -p 33.33.33.20:3260 <span class="p">&amp;</span>ndash<span class="p">;</span>login
</span><span class='line'>  <span class="c"># Logging in to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] (multiple)</span>
</span><span class='line'>  <span class="c"># Login to [iface: default, target: iqn.2006-04.com.iscsi-is-awesome:1, portal: 33.33.33.20,3260] successful.</span>
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Base Cluster Setup</h2>

<ol>
<li>Run the following commands on both nodes
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;h1&gt;install clustering packages&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;yum -y install pcs fence-agents-all lvm2-cluster&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># set hacluster user password</span>
</span><span class='line'>  <span class="nb">echo</span> <span class="p">&amp;</span>ldquo<span class="p">;</span>hacluster<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">|</span> passwd hacluster <span class="p">&amp;</span>ndash<span class="p">;</span>stdin&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># start clustering services</span>
</span><span class='line'>  systemctl start pcsd.service
</span><span class='line'>  systemctl <span class="nb">enable </span>pcsd.service&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the mountpoint directory</span>
</span><span class='line'>  mkdir -p /var/opt/opscode/drbd/data
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Run the following commands on the first cluster node only
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># authorize cluster</span>
</span><span class='line'>  pcs cluster auth backend0 backend1 -u hacluster -p hacluster&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># setup cluster</span>
</span><span class='line'>  pcs cluster setup <span class="p">&amp;</span>ndash<span class="p">;</span>start <span class="p">&amp;</span>ndash<span class="p">;</span>name chef-ha backend0 backend1&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable the cluster and examine status</span>
</span><span class='line'>  pcs cluster <span class="nb">enable</span> <span class="p">&amp;</span>ndash<span class="p">;</span>all
</span><span class='line'>  pcs cluster status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable SCSI fence mode (uses SPC-3)</span>
</span><span class='line'>  pcs stonith create scsi fence_scsi <span class="nv">devices</span><span class="o">=</span>/dev/sdb meta <span class="nv">provides</span><span class="o">=</span>unfencing
</span><span class='line'>  sleep 5
</span><span class='line'>  <span class="c"># this might show stopped, try it a few times until it says started</span>
</span><span class='line'>  pcs stonith show
</span><span class='line'>  <span class="c">#  scsi (stonith:fence_scsi): Started</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. And on the 2nd cluster node:
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  pcs cluster auth backend0 backend1 -u hacluster -p hacluster
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Option 1: Leader Election and LVM Volume failover without CLVM</h2>

<ol>
<li>on the first cluster node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>  pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the LVM PV and VG</span>
</span><span class='line'>  pvcreate /dev/sdb
</span><span class='line'>  vgcreate shared_vg /dev/sdb
</span><span class='line'>  lvcreate -l 80%VG -n ha_lv shared_vg&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c">#   0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active</span>
</span><span class='line'>  lvchange -aey shared_vg
</span><span class='line'>  <span class="c">#   1 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># format the volume</span>
</span><span class='line'>  mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Update the initramfs device on all your cluster nodes, so that the CLVM volume is never auto-mounted:
</span><span class='line'> &lt;/code&gt;bash&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># Set the lvm configuration to not auto-mount anything but the root &amp;ldquo;centos&amp;rdquo; VG</span>
</span><span class='line'>  <span class="c"># yes, the 783 part means the 783rd line of the file :\</span>
</span><span class='line'>  sed -i.bak <span class="p">&amp;</span>ldquo<span class="p">;</span>783s/.*/    <span class="nv">volume_list</span> <span class="o">=</span> <span class="o">[</span> <span class="se">\&quot;</span>centos<span class="se">\&amp;</span>rdquo<span class="p">;</span>, <span class="se">\&amp;</span>ldquo<span class="p">;</span>@&lt;code&gt;hostname -s&lt;/code&gt;<span class="se">\&amp;</span>rdquo<span class="p">;</span> <span class="o">]</span>/<span class="p">&amp;</span>ldquo<span class="p">;</span> /etc/lvm/lvm.conf
</span><span class='line'>  <span class="c"># update initramfs and reboot</span>
</span><span class='line'>  dracut -H -f /boot/initramfs-<span class="k">$(</span>uname -r<span class="k">)</span>.img <span class="k">$(</span>uname -r<span class="k">)</span>
</span><span class='line'>  <span class="c"># shutdown, but use vagrant to start them up</span>
</span><span class='line'>  shutdown -h now
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. <span class="k">then</span> bring both nodes back up
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  vagrant up backend0 backend1
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>4. Add an LVM resource
</span><span class='line'> &lt;/code&gt;bash
</span><span class='line'>  <span class="c"># create resources for the LVM, filesystem and IP</span>
</span><span class='line'>  pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="p">&amp;</span>rdquo<span class="p">;</span>/dev/shared_vg/ha_lv<span class="err">&quot;</span> <span class="nv">directory</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/var/opt/opscode/drbd/data<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">fstype</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>xfs<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  <span class="c">#&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># debugging - only if things are going wrong</span>
</span><span class='line'>  <span class="c"># run lvs on both nodes to ensure it only says active (&amp;ldquo;a&amp;rdquo;) on backend0</span>
</span><span class='line'>  lvs <span class="c"># on backend0</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi-ao&amp;mdash;-  3.20g</span>
</span><span class='line'>  lvs <span class="c"># on backend1</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi&amp;mdash;&amp;mdash;-  3.20g</span>
</span><span class='line'>  <span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>  bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span><span class='line'>  <span class="c"># or</span>
</span><span class='line'>  pcs resource debug-start ha_lv
</span><span class='line'>  pcs resource debug-start chef_data
</span><span class='line'>  pcs resource debug-start backend_vip
</span></code></pre></td></tr></table></div></figure> </p>

<h2>Option 2:  Using CLVMD</h2>

<ol>
<li>On the first node: Enable clvmd and LVM clustering
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs resource create dlm ocf:pacemaker:controld clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs resource create clvmd ocf:heartbeat:clvm clone on-fail<span class="o">=</span>fence <span class="nv">interleave</span><span class="o">=</span><span class="nb">true </span><span class="nv">ordered</span><span class="o">=</span><span class="nb">true</span>
</span><span class='line'>pcs constraint order start dlm-clone <span class="k">then</span> clvmd-clone
</span><span class='line'>pcs constraint colocation add clvmd-clone with dlm-clone&lt;/li&gt;
</span><span class='line'>&lt;/ol&gt;
</span><span class='line'>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># enable LVM clustering with clvmd</span>
</span><span class='line'>  lvmconf <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-cluster
</span><span class='line'>  <span class="c"># stop lvmetad</span>
</span><span class='line'>  killall lvmetad
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>2. Run the following on the second cluster node:
</span><span class='line'> &lt;/code&gt;
</span><span class='line'>  lvmconf <span class="p">&amp;</span>ndash<span class="p">;</span><span class="nb">enable</span>-cluster&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># stop lvmetad</span>
</span><span class='line'>  killall lvmetad
</span><span class='line'>  <span class="c">#</span>
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>3. Back to the first cluster node:
</span><span class='line'> &lt;/code&gt;bash&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># examine cluster status and ensure all resources are Started/Online</span>
</span><span class='line'>  pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># create the LVM PV and VG</span>
</span><span class='line'>  pvcreate /dev/sdb
</span><span class='line'>  vgcreate -cy shared_vg /dev/sdb
</span><span class='line'>  lvcreate -l 80%VG -n ha_lv shared_vg&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># deactivate the shared_vg, and then reactivate it with an exclusive lock</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c"># 0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active</span>
</span><span class='line'>  vgchange -aey shared_vg
</span><span class='line'>  <span class="c"># 1 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># run lvs on both nodes to ensure it only says active (&amp;ldquo;a&amp;rdquo;) on backend0</span>
</span><span class='line'>  lvs <span class="c"># on backend0</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi-a&amp;mdash;&amp;ndash;  3.20g</span>
</span><span class='line'>  lvs <span class="c"># on backend1</span>
</span><span class='line'>  <span class="c">#  LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert</span>
</span><span class='line'>  <span class="c">#  root  centos    -wi-ao&amp;mdash;- 38.48g</span>
</span><span class='line'>  <span class="c">#  swap  centos    -wi-ao&amp;mdash;-  1.03g</span>
</span><span class='line'>  <span class="c">#  ha_lv shared_vg -wi&amp;mdash;&amp;mdash;-  3.20g&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># format the volume and mount it</span>
</span><span class='line'>  mkfs.xfs /dev/shared_vg/ha_lv
</span><span class='line'>  &lt;code&gt;
</span><span class='line'>4. Add an LVM resource <span class="o">(</span>this is where shit gets whacky<span class="o">)</span>
</span><span class='line'> &lt;/code&gt;
</span><span class='line'>  <span class="c"># first unmount and deactivate</span>
</span><span class='line'>  vgchange -an shared_vg
</span><span class='line'>  <span class="c">#  0 logical volume(s) in volume group &amp;ldquo;shared_vg&amp;rdquo; now active&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  pcs resource create ha_lv ocf:heartbeat:LVM <span class="nv">volgrpname</span><span class="o">=</span>shared_vg <span class="nv">exclusive</span><span class="o">=</span><span class="nb">true</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create chef_data Filesystem <span class="nv">device</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/dev/shared_vg/ha_lv<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">directory</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>/var/opt/opscode/drbd/data<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="nv">fstype</span><span class="o">=</span><span class="p">&amp;</span>ldquo<span class="p">;</span>xfs<span class="p">&amp;</span>rdquo<span class="p">;</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha
</span><span class='line'>  pcs resource create backend_vip IPaddr2 <span class="nv">ip</span><span class="o">=</span>33.33.33.5 <span class="nv">cidr_netmask</span><span class="o">=</span><span class="m">24</span> <span class="p">&amp;</span>ndash<span class="p">;</span>group chef_ha&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;  <span class="c"># debugging</span>
</span><span class='line'>  pcs resource debug-start ha_lv
</span><span class='line'>  pcs resource debug-start chef_data
</span><span class='line'>  pcs resource debug-start backend_vip
</span><span class='line'>  <span class="nb">export </span><span class="nv">OCF_RESKEY_volgrpname</span><span class="o">=</span>shared_vg <span class="nv">OCF_RESKEY_exclusive</span><span class="o">=</span><span class="nb">true </span><span class="nv">OCF_ROOT</span><span class="o">=</span>/usr/lib/ocf
</span><span class='line'>  bash -x /usr/lib/ocf/resource.d/heartbeat/LVM start
</span></code></pre></td></tr></table></div></figure> </p>

<p>Now you&rsquo;re done!</p>

<h2>Failing over</h2>

<h3>the pacemaker way</h3>

<ol>
<li>bringing down backend0</li>
<li>Option 1:  shut down backend0 <code>shutdown -h now</code></li>
<li><p>Option 2:  make it a standby via Pacemaker <code>pcs cluster standby backend0</code></p></li>
<li><p>failover of resources should be automatic to the secondary node:
<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
</pre></td><td class='code'><pre><code class='bash'><span class='line'>pcs status&lt;/p&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Cluster name: chef-ha&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Last updated: Sat Mar <span class="m">14</span> 21:55:56 2015&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Last change: Sat Mar <span class="m">14</span> 21:43:21 <span class="m">2015</span> via cibadmin on backend0&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Stack: corosync&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Current DC: backend1 <span class="o">(</span>2<span class="o">)</span> - partition with quorum&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Version: 1.1.10-32.el7_0.1-368c726&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;2 Nodes configured&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;4 Resources configured&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Online: <span class="o">[</span> backend1 <span class="o">]</span>&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;OFFLINE: <span class="o">[</span> backend0 <span class="o">]</span>&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Full list of resources:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;scsi <span class="o">(</span>stonith:fence_scsi<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Resource Group: chef_ha&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;ha_lv  <span class="o">(</span>ocf::heartbeat:LVM<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;chef_data  <span class="o">(</span>ocf::heartbeat:Filesystem<span class="o">)</span>:  Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend_vip  <span class="o">(</span>ocf::heartbeat:IPaddr2<span class="o">)</span>: Started backend1&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;PCSD Status:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend0: Offline&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;backend1: Online&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'><span class="c">#</span>
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;Daemon Status:&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;corosync: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;pacemaker: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;h1&gt;pcsd: active/enabled&lt;/h1&gt;
</span><span class='line'>
</span><span class='line'>&lt;p&gt;<span class="sb">```</span>&lt;/p&gt;&lt;/li&gt;
</span><span class='line'>&lt;li&gt;Bringing backend0 back up and logging in, it won<span class="p">&amp;</span>rsquo<span class="p">;</span>t want to run pacemaker
</span></code></pre></td></tr></table></div></figure> bash
pcs status

<h1>Error: cluster is not currently running on this node</h1>

<p>systemctl start pacemaker</p></li>
</ol>


<p>  # now checking status
  pcs status
  # Cluster name: chef-ha
  # Last updated: Sat Mar 14 22:06:25 2015
  # Last change: Sat Mar 14 21:43:21 2015 via cibadmin on backend0
  # Stack: corosync
  # Current DC: backend1 (2) - partition with quorum
  # Version: 1.1.10-32.el7_0.1-368c726
  # 2 Nodes configured
  # 4 Resources configured
  #
  #
  # Node backend0 (1): pending
  # Online: [ backend1 ]
  #
  # Full list of resources:
  #
  #  scsi (stonith:fence_scsi): Started backend1
  #  Resource Group: chef_ha
  #      ha_lv  (ocf::heartbeat:LVM): Started backend1
  #      chef_data  (ocf::heartbeat:Filesystem):  Started backend1
  #      backend_vip  (ocf::heartbeat:IPaddr2): Started backend1
  #
  # PCSD Status:
  #   backend0: Online
  #   backend1: Online
  #
  # Daemon Status:
  #   corosync: active/enabled
  #   pacemaker: active/enabled
  #   pcsd: active/enabled
  <code>
4. If backend0 is considered standby, you can just unstandby it:
 </code>bash
  pcs cluster unstandby backend0</p>

<p>  pcs status
  # Cluster name: chef-ha
  # Last updated: Sat Mar 14 22:11:53 2015
  # Last change: Sat Mar 14 22:11:52 2015 via crm_attribute on backend0
  # Stack: corosync
  # Current DC: backend1 (2) - partition with quorum
  # Version: 1.1.10-32.el7_0.1-368c726
  # 2 Nodes configured
  # 4 Resources configured
  #
  #
  # Online: [ backend0 backend1 ]
  <code>
5. What happens if you try to mount the disk on the inactive node?
 </code>bash
  # LVM will be nice, but won&rsquo;t let you
  vgchange -aey shared_vg
  # 0 logical volume(s) in volume group &ldquo;shared_vg&rdquo; now active</p>

<p>  lvchange -aey shared_vg/ha_lv
  lvs
  # LV    VG        Attr       LSize   Pool Origin Data%  Move Log Cpy%Sync Convert
  # root  centos    -wi-ao&mdash;-  38.48g
  # swap  centos    -wi-ao&mdash;-   1.03g
  # ha_lv shared_vg -wi&mdash;&mdash;- 816.00m
  <figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
</pre></td><td class='code'><pre><code class=''><span class='line'><span class="nt">&lt;h3&gt;</span>the CLVM way<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;ol&gt;</span>
</span><span class='line'><span class="nt">&lt;li&gt;</span>On the active node
</span></code></pre></td></tr></table></div></figure> 
[root@backend0 ~]# umount /var/opt/opscode/drbd/data
[root@backend0 ~]# lvchange -an shared_vg/ha_lv</li>
</ol>


<p>  # ensure the LV isn&rsquo;t active
  [root@backend0 ~]# lvs
    LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
    root  centos    -wi-ao&mdash;- 38.48g
    swap  centos    -wi-ao&mdash;-  1.03g
    ha_lv shared_vg -wi&mdash;&mdash;-  3.20g
  <code>
2. on the standby node
 </code>bash</p>

<p>  [root@backend1 ~]# lvs
    LV    VG        Attr       LSize  Pool Origin Data%  Move Log Cpy%Sync Convert
    root  centos    -wi-ao&mdash;- 38.48g
    swap  centos    -wi-ao&mdash;-  1.03g
    ha_lv shared_vg -wi&mdash;&mdash;-  3.20g
  [root@backend1 ~]# lvchange -aey shared_vg/ha_lv
  [root@backend1 ~]# mount /dev/shared_vg/ha_lv /var/opt/opscode/drbd/data
  <figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
</pre></td><td class='code'><pre><code class=''><span class='line'><span class="nt">&lt;h3&gt;</span>Forcing yourself to go standby<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>TBD<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h3&gt;</span>Forcing the other node to go standby (STONITH)<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>TBD<span class="nt">&lt;/p&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h2&gt;</span>Troubleshooting<span class="nt">&lt;/h2&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;h3&gt;</span>Do SCSI SPC-3 persistent reservations work on my device?<span class="nt">&lt;/h3&gt;</span>
</span><span class='line'>
</span><span class='line'><span class="nt">&lt;p&gt;</span>good:
</span></code></pre></td></tr></table></div></figure> bash</p>

<h1>exampling talking to the Linux iSCSI service:</h1>

<p>sg_persist &ndash;in &ndash;report-capabilities -v /dev/sdb</p>

<h1>inquiry cdb: 12 00 00 00 24 00</h1>

<h1>LIO-ORG   block_backend     4.0</h1>

<h1>Peripheral device type: disk</h1>

<h1>Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</h1>

<h1>Report capabilities response:</h1>

<h1>Compatible Reservation Handling(CRH): 1</h1>

<h1>Specify Initiator Ports Capable(SIP_C): 1</h1>

<h1>All Target Ports Capable(ATP_C): 1</h1>

<h1>Persist Through Power Loss Capable(PTPL_C): 1</h1>

<h1>Type Mask Valid(TMV): 1</h1>

<h1>Allow Commands: 1</h1>

<h1>Persist Through Power Loss Active(PTPL_A): 0</h1>

<h1>Support indicated in Type mask:</h1>

<h1>Write Exclusive, all registrants: 1</h1>

<h1>Exclusive Access, registrants only: 1</h1>

<h1>Write Exclusive, registrants only: 1</h1>

<h1>Exclusive Access: 1</h1>

<h1>Write Exclusive: 1</h1>

<h1>Exclusive Access, all registrants: 1</h1>

<pre><code>
bad:
</code></pre>

<h1>example using Virtualbox SCSI/SAS/etc disks which are not SPC-compliant:</h1>

<p>sg_persist &ndash;in &ndash;report-capabilities -v /dev/sdb</p>

<h1>inquiry cdb: 12 00 00 00 24 00</h1>

<h1>VBOX      HARDDISK          1.0</h1>

<h1>Peripheral device type: disk</h1>

<h1>Persistent Reservation In cmd: 5e 02 00 00 00 00 00 20 00 00</h1>

<h1>persistent reservation in:  Fixed format, current;  Sense key: Illegal Request</h1>

<h1>Additional sense: Invalid command operation code</h1>

<h1>Info fld=0x0 [0]</h1>

<h1>PR in (Report capabilities): command not supported</h1>

<p>```</p>

<p>TBD</p>
]]></content>
  </entry>
  
</feed>
